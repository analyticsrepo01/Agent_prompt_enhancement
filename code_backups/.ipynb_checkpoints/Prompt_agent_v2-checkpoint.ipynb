{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d4395-297a-409f-931d-22f4c8843538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56d6a687-06ce-4c40-b7b4-4640f489929c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Project ID: my-project-0004-346516\n",
      "Location: us-central1\n",
      "‚úÖ Setup completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# LangGraph Prompt Optimization System\n",
    "# This notebook creates a 4-agent system for prompt optimization using LangGraph\n",
    "\n",
    "# Installation and Setup\n",
    "%pip install --upgrade --quiet langgraph langchain-google-vertexai google-cloud-aiplatform[evaluation] pandas\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, TypedDict, Annotated\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Google Cloud Setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"us-central1\"\n",
    "EXPERIMENT_NAME = \"prompt-optimization-experiment\"\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\"\n",
    "\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Location: {LOCATION}\")\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "import vertexai\n",
    "from vertexai.evaluation import EvalTask, PointwiseMetric, PointwiseMetricPromptTemplate\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-2.0-flash-001\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "222b71ff-e821-4c29-aaa3-98823b05ecdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ State definition completed!\n",
      "‚úÖ LangGraph workflow compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STATE DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "# Define the state that will be passed between agents\n",
    "class PromptOptimizationState(TypedDict):\n",
    "    original_prompt: str\n",
    "    current_prompt: str\n",
    "    test_dataset: List[Dict[str, str]]\n",
    "    agent2_results: List[Dict[str, Any]]\n",
    "    evaluation_results: Dict[str, Any]\n",
    "    enhancement_recommendations: str\n",
    "    enhancement_makes_sense: bool\n",
    "    final_prompt: str\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    iteration_history: List[Dict[str, Any]]  # Track all iterations\n",
    "    should_continue_optimizing: bool\n",
    "\n",
    "print(\"‚úÖ State definition completed!\")\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT 1: DATASET GENERATOR\n",
    "# =============================================================================\n",
    "\n",
    "def agent1_dataset_generator(state: PromptOptimizationState) -> PromptOptimizationState:\n",
    "    \"\"\"\n",
    "    Agent 1: Generates a dataset of input-output pairs for testing the prompt\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Agent 1: Generating test dataset...\")\n",
    "    \n",
    "    original_prompt = state[\"original_prompt\"]\n",
    "    \n",
    "    dataset_generation_prompt = f\"\"\"\n",
    "    You are a dataset generation expert. Given the following prompt, generate 12 diverse input-output pairs that would be good for testing this prompt.\n",
    "    \n",
    "    Original Prompt: {original_prompt}\n",
    "    \n",
    "    Generate 12 test cases with varied inputs that would help evaluate how well this prompt performs. \n",
    "    Each test case should have:\n",
    "    - input: A realistic input scenario\n",
    "    - expected_output: What a good response should look like\n",
    "    \n",
    "    Make the test cases diverse to cover different scenarios, edge cases, and complexity levels.\n",
    "    \n",
    "    Return ONLY a valid JSON array in this format:\n",
    "    [\n",
    "        {{\"input\": \"test input 1\", \"expected_output\": \"expected response 1\"}},\n",
    "        {{\"input\": \"test input 2\", \"expected_output\": \"expected response 2\"}},\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=dataset_generation_prompt)])\n",
    "    \n",
    "    try:\n",
    "        # Extract JSON from response\n",
    "        response_text = response.content.strip()\n",
    "        if response_text.startswith(\"```json\"):\n",
    "            response_text = response_text[7:-3].strip()\n",
    "        elif response_text.startswith(\"```\"):\n",
    "            response_text = response_text[3:-3].strip()\n",
    "        \n",
    "        test_dataset = json.loads(response_text)\n",
    "        \n",
    "        # Validate dataset structure\n",
    "        if not isinstance(test_dataset, list) or len(test_dataset) != 12:\n",
    "            raise ValueError(\"Dataset should be a list of 12 items\")\n",
    "        \n",
    "        for item in test_dataset:\n",
    "            if not isinstance(item, dict) or \"input\" not in item or \"expected_output\" not in item:\n",
    "                raise ValueError(\"Each item should have 'input' and 'expected_output' keys\")\n",
    "        \n",
    "        state[\"test_dataset\"] = test_dataset\n",
    "        print(f\"‚úÖ Generated {len(test_dataset)} test cases\")\n",
    "        \n",
    "        # Save dataset to file\n",
    "        import os\n",
    "        import datetime\n",
    "        \n",
    "        # Create datasets folder if it doesn't exist\n",
    "        os.makedirs(\"datasets\", exist_ok=True)\n",
    "        \n",
    "        # Generate filename with timestamp to avoid overwriting\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"datasets/test_dataset_{timestamp}.json\"\n",
    "        \n",
    "        # Check if file exists and create versioned filename\n",
    "        counter = 1\n",
    "        original_filename = filename\n",
    "        while os.path.exists(filename):\n",
    "            base_name = original_filename.replace(\".json\", \"\")\n",
    "            filename = f\"{base_name}_v{counter}.json\"\n",
    "            counter += 1\n",
    "        \n",
    "        # Save the dataset\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(test_dataset, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Dataset saved to: {filename}\")\n",
    "        \n",
    "        # Display sample test cases\n",
    "        print(\"\\nüìã Sample test cases:\")\n",
    "        for i, case in enumerate(test_dataset[:3]):\n",
    "            print(f\"Case {i+1}:\")\n",
    "            input_text = case['input'] if len(case['input']) <= 100 else case['input'][:100] + \"...\"\n",
    "            expected_text = case['expected_output'] if len(case['expected_output']) <= 100 else case['expected_output'][:100] + \"...\"\n",
    "            print(f\"  Input: {input_text}\")\n",
    "            print(f\"  Expected: {expected_text}\")\n",
    "            print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating dataset: {e}\")\n",
    "        # Fallback: create a simple dataset\n",
    "        state[\"test_dataset\"] = [\n",
    "            {\"input\": f\"Test input {i+1}\", \"expected_output\": f\"Expected output {i+1}\"}\n",
    "            for i in range(12)\n",
    "        ]\n",
    "    \n",
    "    return state\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT 2: PROMPT EXECUTOR\n",
    "# =============================================================================\n",
    "\n",
    "def agent2_prompt_executor(state: PromptOptimizationState) -> PromptOptimizationState:\n",
    "    \"\"\"\n",
    "    Agent 2: Executes the current prompt against all test cases and collects results\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Agent 2: Executing prompt against test dataset...\")\n",
    "    \n",
    "    current_prompt = state[\"current_prompt\"]\n",
    "    test_dataset = state[\"test_dataset\"]\n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_dataset):\n",
    "        print(f\"Processing test case {i+1}/12...\", end=\" \")\n",
    "        \n",
    "        # Apply the current prompt to the test input\n",
    "        full_prompt = f\"{current_prompt}\\n\\nInput: {test_case['input']}\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke([HumanMessage(content=full_prompt)])\n",
    "            actual_output = response.content.strip()\n",
    "            \n",
    "            result = {\n",
    "                \"test_case_id\": i + 1,\n",
    "                \"input\": test_case[\"input\"],\n",
    "                \"expected_output\": test_case[\"expected_output\"],\n",
    "                \"actual_output\": actual_output,\n",
    "                \"prompt_used\": current_prompt\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(\"‚úÖ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            result = {\n",
    "                \"test_case_id\": i + 1,\n",
    "                \"input\": test_case[\"input\"],\n",
    "                \"expected_output\": test_case[\"expected_output\"],\n",
    "                \"actual_output\": f\"ERROR: {str(e)}\",\n",
    "                \"prompt_used\": current_prompt\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    state[\"agent2_results\"] = results\n",
    "    print(f\"‚úÖ Completed execution on {len(results)} test cases\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT 3: EVALUATION ANALYZER\n",
    "# =============================================================================\n",
    "\n",
    "def agent3_evaluation_analyzer(state: PromptOptimizationState) -> PromptOptimizationState:\n",
    "    \"\"\"\n",
    "    Agent 3: Analyzes the results using Vertex AI evaluation and provides enhancement recommendations\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Agent 3: Analyzing results and generating recommendations...\")\n",
    "    \n",
    "    results = state[\"agent2_results\"]\n",
    "    \n",
    "    # Create evaluation dataset\n",
    "    eval_data = []\n",
    "    for result in results:\n",
    "        eval_data.append({\n",
    "            \"input\": result[\"input\"],\n",
    "            \"expected_output\": result[\"expected_output\"],\n",
    "            \"response\": result[\"actual_output\"]  # Changed from actual_output to response\n",
    "        })\n",
    "    \n",
    "    eval_df = pd.DataFrame(eval_data)\n",
    "    \n",
    "    # Define custom evaluation metric for prompt quality\n",
    "    prompt_quality_metric = PointwiseMetric(\n",
    "        metric=\"prompt_quality\",\n",
    "        metric_prompt_template=PointwiseMetricPromptTemplate(\n",
    "            criteria={\n",
    "                \"accuracy\": \"The actual output matches the expected output in terms of correctness and completeness\",\n",
    "                \"relevance\": \"The actual output is relevant to the input and addresses the main points\",\n",
    "                \"clarity\": \"The actual output is clear, well-structured, and easy to understand\",\n",
    "                \"consistency\": \"The output style and format are consistent with expectations\"\n",
    "            },\n",
    "            rating_rubric={\n",
    "                \"5\": \"Excellent: Meets all criteria exceptionally well\",\n",
    "                \"4\": \"Good: Meets most criteria well with minor issues\",\n",
    "                \"3\": \"Average: Meets some criteria but has notable gaps\",\n",
    "                \"2\": \"Poor: Falls short on most criteria\",\n",
    "                \"1\": \"Very Poor: Fails to meet criteria\"\n",
    "            },\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Run evaluation\n",
    "        eval_task = EvalTask(\n",
    "            dataset=eval_df,\n",
    "            metrics=[prompt_quality_metric],\n",
    "            experiment=EXPERIMENT_NAME\n",
    "        )\n",
    "        \n",
    "        eval_result = eval_task.evaluate()\n",
    "        \n",
    "        # Extract evaluation scores\n",
    "        scores = []\n",
    "        if hasattr(eval_result, 'summary_metrics'):\n",
    "            for metric_name, metric_value in eval_result.summary_metrics.items():\n",
    "                scores.append(f\"{metric_name}: {metric_value}\")\n",
    "        \n",
    "        evaluation_summary = \"\\n\".join(scores) if scores else \"Evaluation completed\"\n",
    "        \n",
    "        # Calculate simple metrics as fallback\n",
    "        total_cases = len(results)\n",
    "        success_cases = sum(1 for r in results if \"ERROR\" not in r[\"actual_output\"])\n",
    "        success_rate = (success_cases / total_cases) * 100 if total_cases > 0 else 0\n",
    "        \n",
    "        evaluation_summary += f\"\\nSuccess Rate: {success_rate:.1f}% ({success_cases}/{total_cases})\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Evaluation error: {e}\")\n",
    "        # Fallback evaluation\n",
    "        total_cases = len(results)\n",
    "        success_cases = sum(1 for r in results if \"ERROR\" not in r[\"actual_output\"])\n",
    "        success_rate = (success_cases / total_cases) * 100 if total_cases > 0 else 0\n",
    "        evaluation_summary = f\"Basic Evaluation - Success Rate: {success_rate:.1f}% ({success_cases}/{total_cases})\"\n",
    "    \n",
    "    # Generate enhancement recommendations using LLM\n",
    "    analysis_prompt = f\"\"\"\n",
    "    You are a prompt engineering expert. Analyze the following test results and provide specific recommendations to improve the prompt.\n",
    "    \n",
    "    Original Prompt: {state[\"current_prompt\"]}\n",
    "    \n",
    "    Evaluation Summary: {evaluation_summary}\n",
    "    \n",
    "    Sample Results:\n",
    "    {json.dumps(results[:3], indent=2)}\n",
    "    \n",
    "    Based on this analysis, provide specific, actionable recommendations to enhance the prompt. \n",
    "    Focus on:\n",
    "    1. What patterns of errors or suboptimal responses do you see?\n",
    "    2. How can the prompt be made clearer or more specific?\n",
    "    3. What instructions or examples should be added?\n",
    "    4. What formatting or structure improvements are needed?\n",
    "    \n",
    "    Provide your recommendations in a clear, structured format.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        analysis_response = llm.invoke([HumanMessage(content=analysis_prompt)])\n",
    "        enhancement_recommendations = analysis_response.content.strip()\n",
    "    except Exception as e:\n",
    "        enhancement_recommendations = f\"Error generating recommendations: {e}\"\n",
    "    \n",
    "    state[\"evaluation_results\"] = {\n",
    "        \"summary\": evaluation_summary,\n",
    "        \"total_cases\": len(results),\n",
    "        \"success_cases\": sum(1 for r in results if \"ERROR\" not in r[\"actual_output\"]),\n",
    "        \"detailed_results\": results,\n",
    "        \"success_rate\": success_rate\n",
    "    }\n",
    "    state[\"enhancement_recommendations\"] = enhancement_recommendations\n",
    "    \n",
    "    # Store iteration history\n",
    "    if \"iteration_history\" not in state:\n",
    "        state[\"iteration_history\"] = []\n",
    "    \n",
    "    iteration_data = {\n",
    "        \"iteration\": state.get(\"iteration\", 1),\n",
    "        \"prompt\": state[\"current_prompt\"],\n",
    "        \"success_rate\": success_rate,\n",
    "        \"evaluation_summary\": evaluation_summary,\n",
    "        \"recommendations\": enhancement_recommendations\n",
    "    }\n",
    "    state[\"iteration_history\"].append(iteration_data)\n",
    "    \n",
    "    print(\"‚úÖ Evaluation completed\")\n",
    "    print(f\"üìä {evaluation_summary}\")\n",
    "    print(f\"üìù Recommendations generated\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT 4: ENHANCEMENT VALIDATOR\n",
    "# =============================================================================\n",
    "\n",
    "def agent4_enhancement_validator(state: PromptOptimizationState) -> PromptOptimizationState:\n",
    "    \"\"\"\n",
    "    Agent 4: Validates if enhancement recommendations make sense and creates final prompt\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Agent 4: Validating enhancements and finalizing prompt...\")\n",
    "    \n",
    "    current_prompt = state[\"current_prompt\"]\n",
    "    recommendations = state[\"enhancement_recommendations\"]\n",
    "    evaluation_results = state[\"evaluation_results\"]\n",
    "    iteration = state.get(\"iteration\", 1)\n",
    "    max_iterations = state.get(\"max_iterations\", 5)  # Increased to 5\n",
    "    iteration_history = state.get(\"iteration_history\", [])\n",
    "    \n",
    "    # Force continuation if we haven't reached minimum iterations (at least 3)\n",
    "    min_iterations = 3\n",
    "    current_success_rate = evaluation_results.get(\"success_rate\", 0)\n",
    "    \n",
    "    print(f\"üìä Current iteration: {iteration}/{max_iterations}\")\n",
    "    print(f\"üìà Current success rate: {current_success_rate:.1f}%\")\n",
    "    \n",
    "    # Enhanced validation logic\n",
    "    should_continue = False\n",
    "    \n",
    "    if iteration < min_iterations:\n",
    "        # Always continue for first 3 iterations\n",
    "        should_continue = True\n",
    "        reasoning = f\"Continuing optimization - minimum {min_iterations} iterations required (currently at {iteration})\"\n",
    "    elif iteration < max_iterations:\n",
    "        # Continue if success rate can be improved or if we have room for improvement\n",
    "        if current_success_rate < 90:  # Continue if less than 90% success\n",
    "            should_continue = True\n",
    "            reasoning = f\"Continuing optimization - success rate ({current_success_rate:.1f}%) has room for improvement\"\n",
    "        elif len(iteration_history) >= 2:\n",
    "            # Check if we're improving compared to previous iteration\n",
    "            prev_success_rate = iteration_history[-2].get(\"success_rate\", 0)\n",
    "            if current_success_rate <= prev_success_rate + 5:  # Less than 5% improvement\n",
    "                should_continue = True\n",
    "                reasoning = f\"Continuing optimization - exploring different approaches for better results\"\n",
    "            else:\n",
    "                should_continue = False\n",
    "                reasoning = f\"Good improvement achieved - ready to finalize\"\n",
    "        else:\n",
    "            should_continue = True\n",
    "            reasoning = \"Continuing optimization - need more data points for comparison\"\n",
    "    else:\n",
    "        should_continue = False\n",
    "        reasoning = f\"Reached maximum iterations ({max_iterations}) - time to select best prompt\"\n",
    "    \n",
    "    if should_continue:\n",
    "        # Generate enhanced prompt\n",
    "        enhancement_prompt = f\"\"\"\n",
    "        You are a prompt engineering expert. Create an improved version of the current prompt based on the analysis.\n",
    "        \n",
    "        Current Prompt: {current_prompt}\n",
    "        \n",
    "        Current Performance: {current_success_rate:.1f}% success rate\n",
    "        \n",
    "        Enhancement Recommendations: {recommendations}\n",
    "        \n",
    "        Iteration History:\n",
    "        {json.dumps(iteration_history, indent=2)}\n",
    "        \n",
    "        Create a significantly improved version of the prompt that addresses the identified issues.\n",
    "        Focus on making it more specific, clear, and effective. Make meaningful changes, not just minor tweaks.\n",
    "        \n",
    "        Return only the improved prompt, nothing else.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            enhancement_response = llm.invoke([HumanMessage(content=enhancement_prompt)])\n",
    "            enhanced_prompt = enhancement_response.content.strip()\n",
    "            \n",
    "            state[\"current_prompt\"] = enhanced_prompt\n",
    "            state[\"iteration\"] = iteration + 1\n",
    "            state[\"enhancement_makes_sense\"] = True\n",
    "            state[\"should_continue_optimizing\"] = True\n",
    "            \n",
    "            print(f\"‚úÖ Enhancement approved - Moving to iteration {state['iteration']}\")\n",
    "            print(f\"üí° Reasoning: {reasoning}\")\n",
    "            print(f\"üîÑ Enhanced prompt created\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error enhancing prompt: {e}\")\n",
    "            state[\"enhancement_makes_sense\"] = False\n",
    "            state[\"should_continue_optimizing\"] = False\n",
    "    \n",
    "    else:\n",
    "        # Time to finalize - select best prompt from history\n",
    "        print(\"üéØ Finalizing prompt - analyzing all iterations...\")\n",
    "        \n",
    "        # Find the best performing prompt from history\n",
    "        best_iteration = max(iteration_history, key=lambda x: x.get(\"success_rate\", 0))\n",
    "        \n",
    "        finalization_prompt = f\"\"\"\n",
    "        You are a prompt engineering expert making the final selection. Review all iterations and select/refine the best prompt.\n",
    "        \n",
    "        Original Prompt: {state[\"original_prompt\"]}\n",
    "        \n",
    "        All Iterations Performance:\n",
    "        {json.dumps(iteration_history, indent=2)}\n",
    "        \n",
    "        Best Performing Iteration: {best_iteration[\"iteration\"]} with {best_iteration[\"success_rate\"]:.1f}% success rate\n",
    "        \n",
    "        Based on this analysis, provide the final optimized prompt. You can either:\n",
    "        1. Select the best performing prompt as-is\n",
    "        2. Create a refined version that combines the best elements from multiple iterations\n",
    "        \n",
    "        Consider both performance metrics and the quality of outputs when making your decision.\n",
    "        \n",
    "        Return only the final prompt, nothing else.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            final_response = llm.invoke([HumanMessage(content=finalization_prompt)])\n",
    "            final_prompt = final_response.content.strip()\n",
    "            \n",
    "            state[\"final_prompt\"] = final_prompt\n",
    "            state[\"enhancement_makes_sense\"] = False\n",
    "            state[\"should_continue_optimizing\"] = False\n",
    "            \n",
    "            print(\"‚úÖ Final prompt selected\")\n",
    "            print(f\"üí° Reasoning: {reasoning}\")\n",
    "            print(f\"üèÜ Best iteration was #{best_iteration['iteration']} with {best_iteration['success_rate']:.1f}% success rate\")\n",
    "            \n",
    "            # Save comprehensive results to file\n",
    "            with open(\"readme_prompt.md\", \"w\") as f:\n",
    "                f.write(\"# Final Optimized Prompt\\n\\n\")\n",
    "                f.write(f\"## Original Prompt\\n```\\n{state['original_prompt']}\\n```\\n\\n\")\n",
    "                f.write(f\"## Final Prompt\\n```\\n{final_prompt}\\n```\\n\\n\")\n",
    "                f.write(f\"## Optimization Summary\\n\")\n",
    "                f.write(f\"- Total Iterations: {len(iteration_history)}\\n\")\n",
    "                f.write(f\"- Best Success Rate: {best_iteration['success_rate']:.1f}%\\n\")\n",
    "                f.write(f\"- Best Iteration: #{best_iteration['iteration']}\\n\\n\")\n",
    "                \n",
    "                f.write(f\"## Iteration History\\n\")\n",
    "                for i, iter_data in enumerate(iteration_history):\n",
    "                    f.write(f\"### Iteration {iter_data['iteration']}\\n\")\n",
    "                    f.write(f\"- Success Rate: {iter_data['success_rate']:.1f}%\\n\")\n",
    "                    f.write(f\"- Prompt: ```{iter_data['prompt']}```\\n\")\n",
    "                    f.write(f\"- Recommendations: {iter_data['recommendations'][:200]}...\\n\\n\")\n",
    "                \n",
    "                f.write(f\"## Final Selection Reasoning\\n{reasoning}\\n\")\n",
    "            \n",
    "            print(\"üíæ Comprehensive results saved to readme_prompt.md\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error finalizing prompt: {e}\")\n",
    "            state[\"final_prompt\"] = best_iteration[\"prompt\"]\n",
    "            state[\"enhancement_makes_sense\"] = False\n",
    "            state[\"should_continue_optimizing\"] = False\n",
    "    \n",
    "    return state\n",
    "\n",
    "# =============================================================================\n",
    "# WORKFLOW DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "# Create the workflow graph\n",
    "workflow = StateGraph(PromptOptimizationState)\n",
    "\n",
    "# Add nodes (agents)\n",
    "workflow.add_node(\"dataset_generator\", agent1_dataset_generator)\n",
    "workflow.add_node(\"prompt_executor\", agent2_prompt_executor)\n",
    "workflow.add_node(\"evaluation_analyzer\", agent3_evaluation_analyzer)\n",
    "workflow.add_node(\"enhancement_validator\", agent4_enhancement_validator)\n",
    "\n",
    "# Define the flow\n",
    "workflow.set_entry_point(\"dataset_generator\")\n",
    "\n",
    "# Sequential flow for first iteration\n",
    "workflow.add_edge(\"dataset_generator\", \"prompt_executor\")\n",
    "workflow.add_edge(\"prompt_executor\", \"evaluation_analyzer\") \n",
    "workflow.add_edge(\"evaluation_analyzer\", \"enhancement_validator\")\n",
    "\n",
    "# Conditional flow after validation\n",
    "def should_continue(state: PromptOptimizationState) -> str:\n",
    "    \"\"\"Decide whether to continue optimization or end\"\"\"\n",
    "    should_continue_opt = state.get(\"should_continue_optimizing\", False)\n",
    "    if should_continue_opt and state.get(\"iteration\", 1) <= state.get(\"max_iterations\", 5):\n",
    "        return \"prompt_executor\"  # Continue with another iteration\n",
    "    else:\n",
    "        return END  # Finalize\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"enhancement_validator\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"prompt_executor\": \"prompt_executor\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ LangGraph workflow compiled successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54accd44-937f-4335-bd22-218ea783cef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_prompt = \"\"\"\n",
    "You are a helpful assistant that writes creative product descriptions for e-commerce. \n",
    "Given a product name and basic features, write an engaging product description that highlights key benefits and appeals to customers.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = '''## Context:\n",
    "    Evaluate the relevance and necessity of all the provided medical care in relation to each item in the ICD and surgery descriptions list using your professional medical knowledge.\n",
    "   \n",
    "    ## Instruction:\n",
    "    For each item in the ICD and surgery descriptions list, determine:\n",
    "    If the medical care is necessary for diagnostic or examination purposes.\n",
    "    If the medical care is effective for the disease.\n",
    "    If the benefits of medical care outweigh any risks.\n",
    "    If the medical care is a standard practice for the diagnosis.\n",
    "    If the medical care is essential for the disease and not for cosmetic/lifestyle purposes.\n",
    "    If the medical care indirectly treats the disease.\n",
    "    Medical care that is follow-up visits, repeat visits, repeat consultation, total amount, GST information, subsidies or discounts all conclude as 'yes'. Especially for GST, their descriptions may come in forms such as 'GST - ADD GST', 'GST - LESS GST'; for any descriptions that resemble these, conclude as 'yes'.\n",
    "    Considering the interactions between drugs, some drugs and treatments may not be designed for the patient's diagnoses and surgeries, but rather to counteract the side effects caused by other drugs. this scenario also needs to conclude as 'yes'\n",
    "    Analyse the exclusion details and verify if the treatment is relevant to the provided exclusion. if the treatment is relevant to the exclusion, conclude as 'no'. 'explanation' should be provided as the treatment is relevant to the exclusion list.\n",
    "    Conversely, consider the treatment medically unnecessary if it is ineffective, has safer alternatives, is discouraged by guidelines, or if risks outweigh benefits.\n",
    "   \n",
    "    ## Input: Given Medical Care: {treatments}.\n",
    "    ## ICD and Surgery Description list: {icd_surgery_description}.\n",
    "    ## Exclusion details: {exclusions}.\n",
    "   \n",
    "    ### Question: are all the given medical care at least a relevant testing or treatment for one of the items in the ICD and surgery descriptions list.\n",
    "   \n",
    "    ### Conclusion:\n",
    "    1. Explain the given result as a doctor.\n",
    "    2. Given different patient profiles and medical histories, provide a probability score without an explanation based on your analysis for the Yes/No binary output you provided to indicate the likelihood of this claim getting approved.\n",
    "   \n",
    "    ### Response: MUST Provide the output in json format with a key \"conclusion\" stating yes/no of the relevance, a key \"probability score\" providing a probability score (0-1 scale) to indicate the likelihood of this claim getting approved with 2 decimal places, and another key \"explanation\" stating the explanation as a doctor. Ensure that the conclusion is \"yes\" only if the probability score is at least 0.5. Ensure that the explanation is given in complete sentences without any truncations.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52b40054-d9aa-4ec4-9991-92346f807aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Prompt Optimization Process...\n",
      "üìù Original Prompt: \n",
      "You are a helpful assistant that writes creative product descriptions for e-commerce. \n",
      "Given a product name and basic features, write an engaging product description that highlights key benefits and appeals to customers.\n",
      "\n",
      "================================================================================\n",
      "ü§ñ Agent 1: Generating test dataset...\n",
      "‚úÖ Generated 12 test cases\n",
      "üíæ Dataset saved to: datasets/test_dataset_20250616_154806.json\n",
      "\n",
      "üìã Sample test cases:\n",
      "Case 1:\n",
      "  Input: Product Name: 'CozyCloud Memory Foam Pillow'; Features: Adapts to your head and neck, relieves press...\n",
      "  Expected: Drift off to dreamland with the CozyCloud Memory Foam Pillow! This isn't just a pillow; it's your pe...\n",
      "\n",
      "Case 2:\n",
      "  Input: Product Name: 'AquaBlast Portable Bluetooth Speaker'; Features: Waterproof, Bluetooth 5.0, 10-hour b...\n",
      "  Expected: Take the party anywhere with the AquaBlast Portable Bluetooth Speaker! This rugged and waterproof sp...\n",
      "\n",
      "Case 3:\n",
      "  Input: Product Name: 'EcoBloom Reusable Shopping Bags'; Features: Made from recycled materials, durable, li...\n",
      "  Expected: Shop sustainably in style with EcoBloom Reusable Shopping Bags! These aren't your grandma's flimsy s...\n",
      "\n",
      "ü§ñ Agent 2: Executing prompt against test dataset...\n",
      "Processing test case 1/12... ‚úÖ\n",
      "Processing test case 2/12... ‚úÖ\n",
      "Processing test case 3/12... ‚úÖ\n",
      "Processing test case 4/12... ‚úÖ\n",
      "Processing test case 5/12... ‚úÖ\n",
      "Processing test case 6/12... ‚úÖ\n",
      "Processing test case 7/12... ‚úÖ\n",
      "Processing test case 8/12... ‚úÖ\n",
      "Processing test case 9/12... ‚úÖ\n",
      "Processing test case 10/12... ‚úÖ\n",
      "Processing test case 11/12... ‚úÖ\n",
      "Processing test case 12/12... ‚úÖ\n",
      "‚úÖ Completed execution on 12 test cases\n",
      "ü§ñ Agent 3: Analyzing results and generating recommendations...\n",
      "The `input_variables` parameter is empty. Only the `response` column is used for computing this model-based metric.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-a8014ad8-e39a-4595-86de-f74e18a95be9\" href=\"#view-view-vertex-resource-a8014ad8-e39a-4595-86de-f74e18a95be9\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-a8014ad8-e39a-4595-86de-f74e18a95be9');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/255766800726/locations/us-central1/metadataStores/default/contexts/prompt-optimization-experiment-5c312b15-47e9-416c-bfe6-96a6167336d7 to Experiment: prompt-optimization-experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-7c2ad954-e7f2-4900-a321-eebf405f8070\" href=\"#view-view-vertex-resource-7c2ad954-e7f2-4900-a321-eebf405f8070\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-7c2ad954-e7f2-4900-a321-eebf405f8070');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-5c312b15-47e9-416c-bfe6-96a6167336d7?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-5c312b15-47e9-416c-bfe6-96a6167336d7?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 12 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:01<00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 12 metric requests are successfully computed.\n",
      "Evaluation Took:1.8372830159496516 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-3bdfdc7d-8503-48e8-8990-d630f147b8c5\" href=\"#view-view-vertex-resource-3bdfdc7d-8503-48e8-8990-d630f147b8c5\">\n",
       "          <span class=\"material-icons view-vertex-icon\">bar_chart</span>\n",
       "          <span>View evaluation results</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-3bdfdc7d-8503-48e8-8990-d630f147b8c5');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation');\n",
       "              } else {\n",
       "                window.open('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation completed\n",
      "üìä row_count: 12\n",
      "prompt_quality/mean: 4.916666666666667\n",
      "prompt_quality/std: 0.2886751345948129\n",
      "Success Rate: 100.0% (12/12)\n",
      "üìù Recommendations generated\n",
      "ü§ñ Agent 4: Validating enhancements and finalizing prompt...\n",
      "‚úÖ Finalization approved\n",
      "üí° Reasoning: The recommendations are excellent and address the identified issues of verbosity, inconsistent tone, and unnecessary formatting in the generated product descriptions. The proposed changes are specific, actionable, and likely to improve the prompt's performance by guiding the model towards more concise and professional outputs suitable for e-commerce. The addition of negative constraints and structural guidance is particularly valuable. \n",
      "üíæ Final prompt saved to readme_prompt.md\n",
      "\n",
      "================================================================================\n",
      "üéâ OPTIMIZATION COMPLETE!\n",
      "================================================================================\n",
      "üìä Final Results:\n",
      "   - Total Iterations: 1\n",
      "   - Test Cases: 12\n",
      "   - Success Rate: 12/12\n",
      "\n",
      "üìù Original Prompt:\n",
      "   \n",
      "You are a helpful assistant that writes creative product descriptions for e-commerce. \n",
      "Given a product name and basic features, write an engaging product description that highlights key benefits and appeals to customers.\n",
      "\n",
      "\n",
      "üéØ Final Prompt:\n",
      "   You are a helpful assistant that writes creative product descriptions for e-commerce. Given a product name and basic features, write an engaging product description that highlights key benefits and appeals to customers. The description should be approximately 100-150 words long. Maintain a balance between enthusiasm and professionalism. Avoid overly dramatic or repetitive language. Do not include unnecessary headings or bullet point lists unless the features naturally lend themselves to a list format. The description should briefly introduce the product, highlight 2-3 key benefits, and conclude with a call to action.\n",
      "\n",
      "üíæ Detailed results saved to: readme_prompt.md\n",
      "\n",
      "‚úÖ Process completed!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTION EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ Starting Prompt Optimization Process...\")\n",
    "print(f\"üìù Original Prompt: {user_prompt}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize state\n",
    "initial_state = {\n",
    "    \"original_prompt\": user_prompt,\n",
    "    \"current_prompt\": user_prompt,\n",
    "    \"test_dataset\": [],\n",
    "    \"agent2_results\": [],\n",
    "    \"evaluation_results\": {},\n",
    "    \"enhancement_recommendations\": \"\",\n",
    "    \"enhancement_makes_sense\": False,\n",
    "    \"final_prompt\": \"\",\n",
    "    \"iteration\": 1,\n",
    "    \"max_iterations\": 5,  # Increased to 5\n",
    "    \"iteration_history\": [],\n",
    "    \"should_continue_optimizing\": True\n",
    "}\n",
    "\n",
    "# Run the workflow\n",
    "try:\n",
    "    final_state = app.invoke(initial_state)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ OPTIMIZATION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"üìä Final Results:\")\n",
    "    print(f\"   - Total Iterations: {final_state.get('iteration', 1)}\")\n",
    "    print(f\"   - Test Cases: {len(final_state.get('test_dataset', []))}\")\n",
    "    print(f\"   - Success Rate: {final_state.get('evaluation_results', {}).get('success_cases', 0)}/{final_state.get('evaluation_results', {}).get('total_cases', 0)}\")\n",
    "    \n",
    "    print(f\"\\nüìù Original Prompt:\")\n",
    "    print(f\"   {final_state['original_prompt']}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Final Prompt:\")\n",
    "    print(f\"   {final_state.get('final_prompt', final_state['current_prompt'])}\")\n",
    "    \n",
    "    if os.path.exists(\"readme_prompt.md\"):\n",
    "        print(f\"\\nüíæ Detailed results saved to: readme_prompt.md\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during execution: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n‚úÖ Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f3e53-5229-4ef6-be4c-34e631c46a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py312",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "conda-base-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
