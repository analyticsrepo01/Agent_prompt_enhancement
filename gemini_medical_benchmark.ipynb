{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b696194c-d483-4a31-9e90-32d7d1362bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import JSON\n",
    "import json\n",
    "\n",
    "# Cloud project id.\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]  # @param {type:\"string\"}\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\" # Use Vertex AI API\n",
    "# [your-project-id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81e8bc1-cec8-4cd7-94b3-5de8017c41ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"HERE IS MY PROMPT\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b9bc44-dec6-459b-9259-e8b36a5af05b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_model = model = \"gemini-2.0-flash-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1513d4f4-8482-4b97-8020-bc830db3e7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. I'm ready. Please provide your prompt. I will do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import base64\n",
    "\n",
    "def generate():\n",
    "  client = genai.Client(\n",
    "      vertexai=True,\n",
    "      project=PROJECT_ID,\n",
    "      location=LOCATION,\n",
    "  )\n",
    "\n",
    "  si_text1 = \"\"\"You are adapted to serve as an intelligent medical companion. You are committed to upholding the highest standards of accuracy, reliability, and ethical considerations in healthcare.\n",
    "\n",
    "Reply in the same language as the user request, unless instructed otherwise by the user.\n",
    "\n",
    "Make sure to answer all parts of the user's instructions, unless they compromise safety.\n",
    "\n",
    "For medical documentation, avoid including information that was not present in the source documents.\n",
    "\n",
    "For diagnosis questions, try to provide relevant information but refrain from providing direct medical advice.\"\"\"\n",
    "\n",
    "  model = main_model\n",
    "  contents = [\n",
    "    types.Content(\n",
    "      role=\"user\",\n",
    "      parts=[\n",
    "        types.Part.from_text(text=test_prompt)\n",
    "      ]\n",
    "    )\n",
    "  ]\n",
    "\n",
    "  generate_content_config = types.GenerateContentConfig(\n",
    "    temperature = 1,\n",
    "    top_p = 0.95,\n",
    "    max_output_tokens = 8192,\n",
    "    safety_settings = [types.SafetySetting(\n",
    "      category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
    "      threshold=\"OFF\"\n",
    "    ),types.SafetySetting(\n",
    "      category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "      threshold=\"OFF\"\n",
    "    ),types.SafetySetting(\n",
    "      category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "      threshold=\"OFF\"\n",
    "    ),types.SafetySetting(\n",
    "      category=\"HARM_CATEGORY_HARASSMENT\",\n",
    "      threshold=\"OFF\"\n",
    "    )],\n",
    "    system_instruction=[types.Part.from_text(text=si_text1)],\n",
    "  )\n",
    "\n",
    "  for chunk in client.models.generate_content_stream(\n",
    "    model = model,\n",
    "    contents = contents,\n",
    "    config = generate_content_config,\n",
    "    ):\n",
    "    print(chunk.text, end=\"\")\n",
    "\n",
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a9f2ca0-80c2-40f3-9a50-1ff0c6264ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_medical_model = \"medlm-large-1.5@001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f69b8f-3f97-4cc0-8482-07b69b937d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, SafetySetting, FinishReason\n",
    "\n",
    "\n",
    "def generate():\n",
    "    vertexai.init(\n",
    "        project=PROJECT_ID,\n",
    "        location=\"us-central1\",\n",
    "        api_endpoint=\"us-central1-aiplatform.googleapis.com\"\n",
    "    )\n",
    "    model = GenerativeModel(\n",
    "        main_medical_model,\n",
    "        system_instruction=[si_text1]\n",
    "    )\n",
    "    responses = model.generate_content(\n",
    "        [test_prompt],\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    for response in responses:\n",
    "        print(response.text, end=\"\")\n",
    "\n",
    "si_text1 = \"\"\"You are a medical expert at Insurance firm, try to asnwer the questions medically.\"\"\"\n",
    "\n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE\n",
    "    ),\n",
    "]\n",
    "\n",
    "generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e92ba8-5259-492e-93d5-acb9f8b95fa9",
   "metadata": {},
   "source": [
    "### medgemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f7563-ad01-4acb-9c04-fb6f395aaec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Import deployed model\n",
    "\n",
    "# @markdown To get [online predictions](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions), you will need a MedGemma [Vertex AI Endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment) that has been deployed from Model Garden. If you have not already done so, go to the [MedGemma model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/medgemma) and click \"Deploy options > Vertex AI\" to deploy the model.\n",
    "\n",
    "# @markdown **Note:** The examples in this notebook are intended to be used with instruction-tuned variants. Make sure to use an instruction-tuned model variant to run this notebook.\n",
    "\n",
    "# @markdown This section gets the Vertex AI Endpoint resource that you deployed from Model Garden to use for online predictions.\n",
    "\n",
    "# @markdown Fill in the endpoint ID and region below. You can find your deployed endpoint on the [Vertex AI online prediction page](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
    "\n",
    "ENDPOINT_ID = \"333033275960328192\"  # @param {type: \"string\", placeholder:\"e.g. 123456789\"}\n",
    "ENDPOINT_REGION = \"asia-southeast1\"  # @param {type: \"string\", placeholder:\"e.g. us-central1\"}\n",
    "\n",
    "# @markdown Set `use_dedicated_endpoint` if you are using a [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type) (`True` by default for Model Garden deployments). Uncheck this option for all other endpoint types.\n",
    "\n",
    "use_dedicated_endpoint = True  # @param {type: \"boolean\"}\n",
    "\n",
    "# @markdown Set `is_thinking` to `True` to turn on thinking mode. **Note:** Thinking is supported for the 27B variant only.\n",
    "is_thinking = True  # @param {type: \"boolean\"}\n",
    "\n",
    "endpoints[\"endpoint\"] = aiplatform.Endpoint(\n",
    "    endpoint_name=ENDPOINT_ID,\n",
    "    project=PROJECT_ID,\n",
    "    location=ENDPOINT_REGION,\n",
    ")\n",
    "\n",
    "# Use the endpoint name to check that you are using an appropriate model variant.\n",
    "# These checks are based on the default endpoint name from the Model Garden\n",
    "# deployment settings.\n",
    "ENDPOINT_NAME = endpoints[\"endpoint\"].display_name\n",
    "if \"pt\" in ENDPOINT_NAME:\n",
    "    raise ValueError(\n",
    "        \"The examples in this notebook are intended to be used with \"\n",
    "        \"instruction-tuned variants. Please use an instruction-tuned model.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5f18dec-893f-457c-b850-f38ecd887be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title #### Specify text prompt\n",
    "\n",
    "system_instruction = \"You are a helpful medical assistant.\"\n",
    "prompt = \"How do you differentiate bacterial from viral pneumonia?\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5a380-ceaa-4444-b22a-efcfd29f28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title #### Generate responses from standard prompts\n",
    "\n",
    "# @markdown This section shows how to send [prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions) requests to the endpoint with standard prompts using the Vertex AI SDK.\n",
    "\n",
    "# @markdown Set `raw_response` to `True` to obtain only the model generated response. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
    "raw_response = True  # @param {type: \"boolean\"}\n",
    "\n",
    "# @markdown Click \"Show Code\" to see more details.\n",
    "\n",
    "formatted_prompt = f\"{system_instruction} {prompt} <start_of_image>\"\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0,\n",
    "        \"raw_response\": raw_response,\n",
    "    },\n",
    "]\n",
    "\n",
    "response = endpoints[\"endpoint\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "prediction = response.predictions[0]\n",
    "\n",
    "display(Markdown(prediction))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-py312-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-env-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
