{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d4395-297a-409f-931d-22f4c8843538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56d6a687-06ce-4c40-b7b4-4640f489929c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Project ID: my-project-0004-346516\n",
      "Location: us-central1\n",
      "✅ Setup completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# LangGraph Prompt Optimization System\n",
    "# This notebook creates a 4-agent system for prompt optimization using LangGraph\n",
    "\n",
    "# Installation and Setup\n",
    "%pip install --upgrade --quiet langgraph langchain-google-vertexai google-cloud-aiplatform[evaluation] pandas\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, TypedDict, Annotated\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Google Cloud Setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"us-central1\"\n",
    "EXPERIMENT_NAME = \"prompt-optimization-experiment\"\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\"\n",
    "\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Location: {LOCATION}\")\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "import vertexai\n",
    "from vertexai.evaluation import EvalTask, PointwiseMetric, PointwiseMetricPromptTemplate\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-2.0-flash-001\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "print(\"✅ Setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9975e9-bb00-4105-9768-7f8487974101",
   "metadata": {},
   "source": [
    "### Agent logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "222b71ff-e821-4c29-aaa3-98823b05ecdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ State definition completed!\n",
      "✅ LangGraph workflow compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "# LangGraph Prompt Optimization System\n",
    "# This notebook creates a 4-agent system for prompt optimization using LangGraph\n",
    "\n",
    "# Installation and Setup\n",
    "%pip install --upgrade --quiet langgraph langchain-google-vertexai google-cloud-aiplatform[evaluation] pandas\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, TypedDict, Annotated\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Google Cloud Setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"us-central1\"\n",
    "EXPERIMENT_NAME = \"prompt-optimization-experiment\"\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\"\n",
    "\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Location: {LOCATION}\")\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "import vertexai\n",
    "from vertexai.evaluation import EvalTask, PointwiseMetric, PointwiseMetricPromptTemplate\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "print(\"✅ Setup completed successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# STATE DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "# Define the state that will be passed between agents\n",
    "class PromptOptimizationState(TypedDict):\n",
    "    original_prompt: str\n",
    "    current_prompt: str\n",
    "    test_dataset: List[Dict[str, str]]\n",
    "    agent2_results: List[Dict[str, Any]]\n",
    "    evaluation_results: Dict[str, Any]\n",
    "    enhancement_recommendations: str\n",
    "    enhancement_makes_sense: bool\n",
    "    final_prompt: str\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    iteration_history: List[Dict[str, Any]]  # Track all iterations\n",
    "    should_continue_optimizing: bool\n",
    "\n",
    "print(\"✅ State definition completed!\")\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT 1: DATASET GENERATOR\n",
    "# =============================================================================\n",
    "\n",
    "def agent1_dataset_generator(state: PromptOptimizationState) -> PromptOptimizationState:\n",
    "    \"\"\"\n",
    "    Agent 1: Generates a dataset of input-output pairs for testing the prompt\n",
    "    Creates challenging test cases to ensure original prompt doesn't get 100% success rate\n",
    "    \"\"\"\n",
    "    print(\"🤖 Agent 1: Generating test dataset...\")\n",
    "    \n",
    "    original_prompt = state[\"original_prompt\"]\n",
    "    \n",
    "    # First, create an initial dataset\n",
    "    dataset_generation_prompt = f\"\"\"\n",
    "    You are a dataset generation expert. Given the following prompt, generate 6 diverse input-output pairs that would be good for testing this prompt.\n",
    "    \n",
    "    Original Prompt: {original_prompt}\n",
    "    \n",
    "    Generate 6 test cases with varied inputs that would help evaluate how well this prompt performs. \n",
    "    Each test case should have:\n",
    "    - input: A realistic input scenario\n",
    "    - expected_output: What a good response should look like\n",
    "    \n",
    "    Make the test cases diverse to cover different scenarios and complexity levels.\n",
    "    \n",
    "    Return ONLY a valid JSON array in this format:\n",
    "    [\n",
    "        {{\"input\": \"test input 1\", \"expected_output\": \"expected response 1\"}},\n",
    "        {{\"input\": \"test input 2\", \"expected_output\": \"expected response 2\"}},\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=dataset_generation_prompt)])\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        response_text = response.content.strip()\n",
    "        if response_text.startswith(\"```json\"):\n",
    "            response_text = response_text[7:-3].strip()\n",
    "        elif response_text.startswith(\"```\"):\n",
    "            response_text = response_text[3:-3].strip()\n",
    "        \n",
    "        initial_dataset = json.loads(response_text)\n",
    "        print(f\"✅ Generated initial {len(initial_dataset)} test cases\")\n",
    "        \n",
    "        # Test the original prompt on initial dataset\n",
    "        print(\"🧪 Testing original prompt on initial dataset...\")\n",
    "        test_results = []\n",
    "        success_count = 0\n",
    "        \n",
    "        for i, test_case in enumerate(initial_dataset):\n",
    "            full_prompt = f\"{original_prompt}\\n\\nInput: {test_case['input']}\"\n",
    "            \n",
    "            try:\n",
    "                test_response = llm.invoke([HumanMessage(content=full_prompt)])\n",
    "                actual_output = test_response.content.strip()\n",
    "                \n",
    "                # Simple success check - if output is not empty and doesn't contain obvious errors\n",
    "                is_success = (len(actual_output) > 10 and \n",
    "                             \"ERROR\" not in actual_output.upper() and\n",
    "                             \"I cannot\" not in actual_output and\n",
    "                             \"I don't know\" not in actual_output)\n",
    "                \n",
    "                if is_success:\n",
    "                    success_count += 1\n",
    "                \n",
    "                test_results.append({\n",
    "                    \"input\": test_case[\"input\"],\n",
    "                    \"expected\": test_case[\"expected_output\"],\n",
    "                    \"actual\": actual_output,\n",
    "                    \"success\": is_success\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                test_results.append({\n",
    "                    \"input\": test_case[\"input\"],\n",
    "                    \"expected\": test_case[\"expected_output\"],\n",
    "                    \"actual\": f\"ERROR: {e}\",\n",
    "                    \"success\": False\n",
    "                })\n",
    "        \n",
    "        initial_success_rate = (success_count / len(initial_dataset)) * 100 if initial_dataset else 0\n",
    "        print(f\"📊 Initial prompt success rate: {initial_success_rate:.1f}% ({success_count}/{len(initial_dataset)})\")\n",
    "        \n",
    "        # If success rate is too high (>85%), generate more challenging cases\n",
    "        final_dataset = initial_dataset.copy()\n",
    "        \n",
    "        if initial_success_rate > 85:\n",
    "            print(\"⚡ Success rate too high - generating challenging test cases...\")\n",
    "            \n",
    "            challenging_prompt = f\"\"\"\n",
    "            The original prompt achieved {initial_success_rate:.1f}% success rate, which is too high for proper testing.\n",
    "            \n",
    "            Original Prompt: {original_prompt}\n",
    "            \n",
    "            Current test results:\n",
    "            {json.dumps(test_results[:3], indent=2)}\n",
    "            \n",
    "            Generate 6 MORE CHALLENGING test cases that will likely expose weaknesses in the original prompt.\n",
    "            These should be:\n",
    "            1. Edge cases or unusual inputs\n",
    "            2. Complex scenarios requiring nuanced responses\n",
    "            3. Ambiguous inputs that could be interpreted multiple ways\n",
    "            4. Cases that test the limits of the prompt's specificity\n",
    "            5. Scenarios with missing or incomplete information\n",
    "            6. Cases that require domain-specific knowledge or reasoning\n",
    "            \n",
    "            Make these significantly more difficult than the initial cases to bring the overall success rate down to 70-80%.\n",
    "            \n",
    "            Return ONLY a valid JSON array with 6 challenging test cases:\n",
    "            [\n",
    "                {{\"input\": \"challenging input 1\", \"expected_output\": \"expected response 1\"}},\n",
    "                {{\"input\": \"challenging input 2\", \"expected_output\": \"expected response 2\"}},\n",
    "                ...\n",
    "            ]\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                challenging_response = llm.invoke([HumanMessage(content=challenging_prompt)])\n",
    "                challenging_text = challenging_response.content.strip()\n",
    "                \n",
    "                if challenging_text.startswith(\"```json\"):\n",
    "                    challenging_text = challenging_text[7:-3].strip()\n",
    "                elif challenging_text.startswith(\"```\"):\n",
    "                    challenging_text = challenging_text[3:-3].strip()\n",
    "                \n",
    "                challenging_dataset = json.loads(challenging_text)\n",
    "                print(f\"✅ Generated {len(challenging_dataset)} challenging test cases\")\n",
    "                \n",
    "                # Combine datasets\n",
    "                final_dataset.extend(challenging_dataset)\n",
    "                \n",
    "                # Test combined dataset\n",
    "                print(\"🧪 Testing original prompt on combined dataset...\")\n",
    "                combined_success = 0\n",
    "                \n",
    "                for test_case in challenging_dataset:\n",
    "                    full_prompt = f\"{original_prompt}\\n\\nInput: {test_case['input']}\"\n",
    "                    \n",
    "                    try:\n",
    "                        test_response = llm.invoke([HumanMessage(content=full_prompt)])\n",
    "                        actual_output = test_response.content.strip()\n",
    "                        \n",
    "                        is_success = (len(actual_output) > 10 and \n",
    "                                     \"ERROR\" not in actual_output.upper() and\n",
    "                                     \"I cannot\" not in actual_output and\n",
    "                                     \"I don't know\" not in actual_output)\n",
    "                        \n",
    "                        if is_success:\n",
    "                            combined_success += 1\n",
    "                            \n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                total_success = success_count + combined_success\n",
    "                combined_success_rate = (total_success / len(final_dataset)) * 100\n",
    "                print(f\"📊 Combined dataset success rate: {combined_success_rate:.1f}% ({total_success}/{len(final_dataset)})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error generating challenging cases: {e}\")\n",
    "                # If challenging generation fails, manually add some generic challenging cases\n",
    "                generic_challenging = [\n",
    "                    {\"input\": \"Handle this ambiguous request with missing context\", \"expected_output\": \"Should ask for clarification while providing helpful guidance\"},\n",
    "                    {\"input\": \"Process this edge case scenario that wasn't covered in training\", \"expected_output\": \"Should gracefully handle unknown scenarios\"},\n",
    "                    {\"input\": \"Deal with conflicting requirements in this complex situation\", \"expected_output\": \"Should prioritize and explain trade-offs\"},\n",
    "                    {\"input\": \"Respond to this intentionally vague and incomplete query\", \"expected_output\": \"Should seek clarification while offering relevant suggestions\"},\n",
    "                    {\"input\": \"Handle this unusual format that doesn't match expected patterns\", \"expected_output\": \"Should adapt to unusual formats while maintaining quality\"},\n",
    "                    {\"input\": \"Process this request that requires domain expertise you might lack\", \"expected_output\": \"Should acknowledge limitations while providing best effort response\"}\n",
    "                ]\n",
    "                final_dataset.extend(generic_challenging)\n",
    "                print(f\"✅ Added {len(generic_challenging)} generic challenging cases\")\n",
    "        \n",
    "        # Validate final dataset structure\n",
    "        if not isinstance(final_dataset, list):\n",
    "            raise ValueError(\"Dataset should be a list\")\n",
    "        \n",
    "        for item in final_dataset:\n",
    "            if not isinstance(item, dict) or \"input\" not in item or \"expected_output\" not in item:\n",
    "                raise ValueError(\"Each item should have 'input' and 'expected_output' keys\")\n",
    "        \n",
    "        state[\"test_dataset\"] = final_dataset\n",
    "        print(f\"✅ Final dataset ready with {len(final_dataset)} test cases\")\n",
    "        \n",
    "        # Save dataset to file\n",
    "        import os\n",
    "        import datetime\n",
    "        \n",
    "        # Create datasets folder if it doesn't exist\n",
    "        os.makedirs(\"datasets\", exist_ok=True)\n",
    "        \n",
    "        # Generate filename with timestamp to avoid overwriting\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"datasets/test_dataset_{timestamp}.json\"\n",
    "        \n",
    "        # Check if file exists and create versioned filename\n",
    "        counter = 1\n",
    "        original_filename = filename\n",
    "        while os.path.exists(filename):\n",
    "            base_name = original_filename.replace(\".json\", \"\")\n",
    "            filename = f\"{base_name}_v{counter}.json\"\n",
    "            counter += 1\n",
    "        \n",
    "        # Save the dataset with metadata\n",
    "        dataset_with_metadata = {\n",
    "            \"metadata\": {\n",
    "                \"original_prompt\": original_prompt,\n",
    "                \"total_cases\": len(final_dataset),\n",
    "                \"initial_success_rate\": initial_success_rate,\n",
    "                \"generation_timestamp\": timestamp\n",
    "            },\n",
    "            \"test_cases\": final_dataset\n",
    "        }\n",
    "        \n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(dataset_with_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"💾 Dataset saved to: {filename}\")\n",
    "        \n",
    "        # Display sample test cases\n",
    "        print(\"\\n📋 Sample test cases:\")\n",
    "        for i, case in enumerate(final_dataset[:3]):\n",
    "            print(f\"Case {i+1}:\")\n",
    "            input_text = case['input'] if len(case['input']) <= 100 else case['input'][:100] + \"...\"\n",
    "            expected_text = case['expected_output'] if len(case['expected_output']) <= 100 else case['expected_output'][:100] + \"...\"\n",
    "            print(f\"  Input: {input_text}\")\n",
    "            print(f\"  Expected: {expected_text}\")\n",
    "            print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating dataset: {e}\")\n",
    "        # Fallback: create a challenging dataset\n",
    "        state[\"test_dataset\"] = [\n",
    "            {\"input\": \"Handle this complex multi-part request with unclear requirements\", \"expected_output\": \"Should break down the request and ask for clarification\"},\n",
    "            {\"input\": \"Process this edge case that wasn't covered in the original prompt\", \"expected_output\": \"Should adapt and provide best effort response\"},\n",
    "            {\"input\": \"Deal with conflicting instructions in this scenario\", \"expected_output\": \"Should identify conflicts and prioritize appropriately\"},\n",
    "            {\"input\": \"Respond to this ambiguous query with missing context\", \"expected_output\": \"Should seek clarification while providing helpful guidance\"},\n",
    "            {\"input\": \"Handle this unusual format input\", \"expected_output\": \"Should adapt to format while maintaining response quality\"},\n",
    "            {\"input\": \"Process this request requiring specialized knowledge\", \"expected_output\": \"Should acknowledge limitations and provide general guidance\"},\n",
    "            {\"input\": \"Basic straightforward request\", \"expected_output\": \"Should handle this easily with clear response\"},\n",
    "            {\"input\": \"Another standard case\", \"expected_output\": \"Should respond appropriately\"},\n",
    "            {\"input\": \"Simple test case\", \"expected_output\": \"Should work well\"},\n",
    "            {\"input\": \"Regular scenario\", \"expected_output\": \"Should handle normally\"},\n",
    "            {\"input\": \"Standard input\", \"expected_output\": \"Should process correctly\"},\n",
    "            {\"input\": \"Normal case\", \"expected_output\": \"Should respond properly\"}\n",
    "        ]\n",
    "        print(f\"✅ Using fallback dataset with {len(state['test_dataset'])} cases\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT 2: PROMPT EXECUTOR\n",
    "# =============================================================================\n",
    "\n",
    "def agent2_prompt_executor(state: PromptOptimizationState) -> PromptOptimizationState:\n",
    "    \"\"\"\n",
    "    Agent 2: Executes the current prompt against all test cases and collects results\n",
    "    \"\"\"\n",
    "    print(\"🤖 Agent 2: Executing prompt against test dataset...\")\n",
    "    \n",
    "    current_prompt = state[\"current_prompt\"]\n",
    "    test_dataset = state[\"test_dataset\"]\n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_dataset):\n",
    "        print(f\"Processing test case {i+1}/12...\", end=\" \")\n",
    "        \n",
    "        # Apply the current prompt to the test input\n",
    "        full_prompt = f\"{current_prompt}\\n\\nInput: {test_case['input']}\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke([HumanMessage(content=full_prompt)])\n",
    "            actual_output = response.content.strip()\n",
    "            \n",
    "            result = {\n",
    "                \"test_case_id\": i + 1,\n",
    "                \"input\": test_case[\"input\"],\n",
    "                \"expected_output\": test_case[\"expected_output\"],\n",
    "                \"actual_output\": actual_output,\n",
    "                \"prompt_used\": current_prompt\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(\"✅\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            result = {\n",
    "                \"test_case_id\": i + 1,\n",
    "                \"input\": test_case[\"input\"],\n",
    "                \"expected_output\": test_case[\"expected_output\"],\n",
    "                \"actual_output\": f\"ERROR: {str(e)}\",\n",
    "                \"prompt_used\": current_prompt\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    state[\"agent2_results\"] = results\n",
    "    print(f\"✅ Completed execution on {len(results)} test cases\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT 3: EVALUATION ANALYZER\n",
    "# =============================================================================\n",
    "\n",
    "def agent3_evaluation_analyzer(state: PromptOptimizationState) -> PromptOptimizationState:\n",
    "    \"\"\"\n",
    "    Agent 3: Analyzes the results using Vertex AI evaluation and provides enhancement recommendations\n",
    "    \"\"\"\n",
    "    print(\"🤖 Agent 3: Analyzing results and generating recommendations...\")\n",
    "    \n",
    "    results = state[\"agent2_results\"]\n",
    "    \n",
    "    # Create evaluation dataset\n",
    "    eval_data = []\n",
    "    for result in results:\n",
    "        eval_data.append({\n",
    "            \"input\": result[\"input\"],\n",
    "            \"expected_output\": result[\"expected_output\"],\n",
    "            \"response\": result[\"actual_output\"]  # Changed from actual_output to response\n",
    "        })\n",
    "    \n",
    "    eval_df = pd.DataFrame(eval_data)\n",
    "    \n",
    "    # Define custom evaluation metric for prompt quality\n",
    "    prompt_quality_metric = PointwiseMetric(\n",
    "        metric=\"prompt_quality\",\n",
    "        metric_prompt_template=PointwiseMetricPromptTemplate(\n",
    "            criteria={\n",
    "                \"accuracy\": \"The actual output matches the expected output in terms of correctness and completeness\",\n",
    "                \"relevance\": \"The actual output is relevant to the input and addresses the main points\",\n",
    "                \"clarity\": \"The actual output is clear, well-structured, and easy to understand\",\n",
    "                \"consistency\": \"The output style and format are consistent with expectations\"\n",
    "            },\n",
    "            rating_rubric={\n",
    "                \"5\": \"Excellent: Meets all criteria exceptionally well\",\n",
    "                \"4\": \"Good: Meets most criteria well with minor issues\",\n",
    "                \"3\": \"Average: Meets some criteria but has notable gaps\",\n",
    "                \"2\": \"Poor: Falls short on most criteria\",\n",
    "                \"1\": \"Very Poor: Fails to meet criteria\"\n",
    "            },\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Run evaluation\n",
    "        eval_task = EvalTask(\n",
    "            dataset=eval_df,\n",
    "            metrics=[prompt_quality_metric],\n",
    "            experiment=EXPERIMENT_NAME\n",
    "        )\n",
    "        \n",
    "        eval_result = eval_task.evaluate()\n",
    "        \n",
    "        # Extract evaluation scores\n",
    "        scores = []\n",
    "        if hasattr(eval_result, 'summary_metrics'):\n",
    "            for metric_name, metric_value in eval_result.summary_metrics.items():\n",
    "                scores.append(f\"{metric_name}: {metric_value}\")\n",
    "        \n",
    "        evaluation_summary = \"\\n\".join(scores) if scores else \"Evaluation completed\"\n",
    "        \n",
    "        # Calculate simple metrics as fallback\n",
    "        total_cases = len(results)\n",
    "        success_cases = sum(1 for r in results if \"ERROR\" not in r[\"actual_output\"])\n",
    "        success_rate = (success_cases / total_cases) * 100 if total_cases > 0 else 0\n",
    "        \n",
    "        evaluation_summary += f\"\\nSuccess Rate: {success_rate:.1f}% ({success_cases}/{total_cases})\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Evaluation error: {e}\")\n",
    "        # Fallback evaluation\n",
    "        total_cases = len(results)\n",
    "        success_cases = sum(1 for r in results if \"ERROR\" not in r[\"actual_output\"])\n",
    "        success_rate = (success_cases / total_cases) * 100 if total_cases > 0 else 0\n",
    "        evaluation_summary = f\"Basic Evaluation - Success Rate: {success_rate:.1f}% ({success_cases}/{total_cases})\"\n",
    "    \n",
    "    # Generate enhancement recommendations using LLM\n",
    "    analysis_prompt = f\"\"\"\n",
    "    You are a prompt engineering expert. Analyze the following test results and provide specific recommendations to improve the prompt.\n",
    "    \n",
    "    Original Prompt: {state[\"current_prompt\"]}\n",
    "    \n",
    "    Evaluation Summary: {evaluation_summary}\n",
    "    \n",
    "    Sample Results:\n",
    "    {json.dumps(results[:3], indent=2)}\n",
    "    \n",
    "    Based on this analysis, provide specific, actionable recommendations to enhance the prompt. \n",
    "    Focus on:\n",
    "    1. What patterns of errors or suboptimal responses do you see?\n",
    "    2. How can the prompt be made clearer or more specific?\n",
    "    3. What instructions or examples should be added?\n",
    "    4. What formatting or structure improvements are needed?\n",
    "    \n",
    "    Provide your recommendations in a clear, structured format.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        analysis_response = llm.invoke([HumanMessage(content=analysis_prompt)])\n",
    "        enhancement_recommendations = analysis_response.content.strip()\n",
    "    except Exception as e:\n",
    "        enhancement_recommendations = f\"Error generating recommendations: {e}\"\n",
    "    \n",
    "    state[\"evaluation_results\"] = {\n",
    "        \"summary\": evaluation_summary,\n",
    "        \"total_cases\": len(results),\n",
    "        \"success_cases\": sum(1 for r in results if \"ERROR\" not in r[\"actual_output\"]),\n",
    "        \"detailed_results\": results,\n",
    "        \"success_rate\": success_rate\n",
    "    }\n",
    "    state[\"enhancement_recommendations\"] = enhancement_recommendations\n",
    "    \n",
    "    # Store iteration history\n",
    "    if \"iteration_history\" not in state:\n",
    "        state[\"iteration_history\"] = []\n",
    "    \n",
    "    iteration_data = {\n",
    "        \"iteration\": state.get(\"iteration\", 1),\n",
    "        \"prompt\": state[\"current_prompt\"],\n",
    "        \"success_rate\": success_rate,\n",
    "        \"evaluation_summary\": evaluation_summary,\n",
    "        \"recommendations\": enhancement_recommendations\n",
    "    }\n",
    "    state[\"iteration_history\"].append(iteration_data)\n",
    "    \n",
    "    print(\"✅ Evaluation completed\")\n",
    "    print(f\"📊 {evaluation_summary}\")\n",
    "    print(f\"📝 Recommendations generated\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT 4: ENHANCEMENT VALIDATOR\n",
    "# =============================================================================\n",
    "\n",
    "def agent4_enhancement_validator(state: PromptOptimizationState) -> PromptOptimizationState:\n",
    "    \"\"\"\n",
    "    Agent 4: Validates if enhancement recommendations make sense and creates final prompt\n",
    "    \"\"\"\n",
    "    print(\"🤖 Agent 4: Validating enhancements and finalizing prompt...\")\n",
    "    \n",
    "    current_prompt = state[\"current_prompt\"]\n",
    "    recommendations = state[\"enhancement_recommendations\"]\n",
    "    evaluation_results = state[\"evaluation_results\"]\n",
    "    iteration = state.get(\"iteration\", 1)\n",
    "    max_iterations = state.get(\"max_iterations\", 5)  # Increased to 5\n",
    "    iteration_history = state.get(\"iteration_history\", [])\n",
    "    \n",
    "    # Force continuation if we haven't reached minimum iterations (at least 3)\n",
    "    min_iterations = 3\n",
    "    current_success_rate = evaluation_results.get(\"success_rate\", 0)\n",
    "    \n",
    "    print(f\"📊 Current iteration: {iteration}/{max_iterations}\")\n",
    "    print(f\"📈 Current success rate: {current_success_rate:.1f}%\")\n",
    "    \n",
    "    # Enhanced validation logic\n",
    "    should_continue = False\n",
    "    \n",
    "    if iteration < min_iterations:\n",
    "        # Always continue for first 3 iterations\n",
    "        should_continue = True\n",
    "        reasoning = f\"Continuing optimization - minimum {min_iterations} iterations required (currently at {iteration})\"\n",
    "    elif iteration < max_iterations:\n",
    "        # Continue if success rate can be improved or if we have room for improvement\n",
    "        if current_success_rate < 90:  # Continue if less than 90% success\n",
    "            should_continue = True\n",
    "            reasoning = f\"Continuing optimization - success rate ({current_success_rate:.1f}%) has room for improvement\"\n",
    "        elif len(iteration_history) >= 2:\n",
    "            # Check if we're improving compared to previous iteration\n",
    "            prev_success_rate = iteration_history[-2].get(\"success_rate\", 0)\n",
    "            if current_success_rate <= prev_success_rate + 5:  # Less than 5% improvement\n",
    "                should_continue = True\n",
    "                reasoning = f\"Continuing optimization - exploring different approaches for better results\"\n",
    "            else:\n",
    "                should_continue = False\n",
    "                reasoning = f\"Good improvement achieved - ready to finalize\"\n",
    "        else:\n",
    "            should_continue = True\n",
    "            reasoning = \"Continuing optimization - need more data points for comparison\"\n",
    "    else:\n",
    "        should_continue = False\n",
    "        reasoning = f\"Reached maximum iterations ({max_iterations}) - time to select best prompt\"\n",
    "    \n",
    "    if should_continue:\n",
    "        # Generate enhanced prompt\n",
    "        enhancement_prompt = f\"\"\"\n",
    "        You are a prompt engineering expert. Create an improved version of the current prompt based on the analysis.\n",
    "        \n",
    "        Current Prompt: {current_prompt}\n",
    "        \n",
    "        Current Performance: {current_success_rate:.1f}% success rate\n",
    "        \n",
    "        Enhancement Recommendations: {recommendations}\n",
    "        \n",
    "        Iteration History:\n",
    "        {json.dumps(iteration_history, indent=2)}\n",
    "        \n",
    "        Create a significantly improved version of the prompt that addresses the identified issues.\n",
    "        Focus on making it more specific, clear, and effective. Make meaningful changes, not just minor tweaks.\n",
    "        \n",
    "        Return only the improved prompt, nothing else.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            enhancement_response = llm.invoke([HumanMessage(content=enhancement_prompt)])\n",
    "            enhanced_prompt = enhancement_response.content.strip()\n",
    "            \n",
    "            state[\"current_prompt\"] = enhanced_prompt\n",
    "            state[\"iteration\"] = iteration + 1\n",
    "            state[\"enhancement_makes_sense\"] = True\n",
    "            state[\"should_continue_optimizing\"] = True\n",
    "            \n",
    "            print(f\"✅ Enhancement approved - Moving to iteration {state['iteration']}\")\n",
    "            print(f\"💡 Reasoning: {reasoning}\")\n",
    "            print(f\"🔄 Enhanced prompt created\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error enhancing prompt: {e}\")\n",
    "            state[\"enhancement_makes_sense\"] = False\n",
    "            state[\"should_continue_optimizing\"] = False\n",
    "    \n",
    "    else:\n",
    "        # Time to finalize - select best prompt from history\n",
    "        print(\"🎯 Finalizing prompt - analyzing all iterations...\")\n",
    "        \n",
    "        # Find the best performing prompt from history\n",
    "        best_iteration = max(iteration_history, key=lambda x: x.get(\"success_rate\", 0))\n",
    "        \n",
    "        finalization_prompt = f\"\"\"\n",
    "        You are a prompt engineering expert making the final selection. Review all iterations and select/refine the best prompt.\n",
    "        \n",
    "        Original Prompt: {state[\"original_prompt\"]}\n",
    "        \n",
    "        All Iterations Performance:\n",
    "        {json.dumps(iteration_history, indent=2)}\n",
    "        \n",
    "        Best Performing Iteration: {best_iteration[\"iteration\"]} with {best_iteration[\"success_rate\"]:.1f}% success rate\n",
    "        \n",
    "        Based on this analysis, provide the final optimized prompt. You can either:\n",
    "        1. Select the best performing prompt as-is\n",
    "        2. Create a refined version that combines the best elements from multiple iterations\n",
    "        \n",
    "        Consider both performance metrics and the quality of outputs when making your decision.\n",
    "        \n",
    "        Return only the final prompt, nothing else.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            final_response = llm.invoke([HumanMessage(content=finalization_prompt)])\n",
    "            final_prompt = final_response.content.strip()\n",
    "            \n",
    "            state[\"final_prompt\"] = final_prompt\n",
    "            state[\"enhancement_makes_sense\"] = False\n",
    "            state[\"should_continue_optimizing\"] = False\n",
    "            \n",
    "            print(\"✅ Final prompt selected\")\n",
    "            print(f\"💡 Reasoning: {reasoning}\")\n",
    "            print(f\"🏆 Best iteration was #{best_iteration['iteration']} with {best_iteration['success_rate']:.1f}% success rate\")\n",
    "            \n",
    "            # Save comprehensive results to file\n",
    "            with open(\"readme_prompt.md\", \"w\") as f:\n",
    "                f.write(\"# Final Optimized Prompt\\n\\n\")\n",
    "                f.write(f\"## Original Prompt\\n```\\n{state['original_prompt']}\\n```\\n\\n\")\n",
    "                f.write(f\"## Final Prompt\\n```\\n{final_prompt}\\n```\\n\\n\")\n",
    "                f.write(f\"## Optimization Summary\\n\")\n",
    "                f.write(f\"- Total Iterations: {len(iteration_history)}\\n\")\n",
    "                f.write(f\"- Best Success Rate: {best_iteration['success_rate']:.1f}%\\n\")\n",
    "                f.write(f\"- Best Iteration: #{best_iteration['iteration']}\\n\\n\")\n",
    "                \n",
    "                f.write(f\"## Iteration History\\n\")\n",
    "                for i, iter_data in enumerate(iteration_history):\n",
    "                    f.write(f\"### Iteration {iter_data['iteration']}\\n\")\n",
    "                    f.write(f\"- Success Rate: {iter_data['success_rate']:.1f}%\\n\")\n",
    "                    f.write(f\"- Prompt: ```{iter_data['prompt']}```\\n\")\n",
    "                    f.write(f\"- Recommendations: {iter_data['recommendations'][:200]}...\\n\\n\")\n",
    "                \n",
    "                f.write(f\"## Final Selection Reasoning\\n{reasoning}\\n\")\n",
    "            \n",
    "            print(\"💾 Comprehensive results saved to readme_prompt.md\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error finalizing prompt: {e}\")\n",
    "            state[\"final_prompt\"] = best_iteration[\"prompt\"]\n",
    "            state[\"enhancement_makes_sense\"] = False\n",
    "            state[\"should_continue_optimizing\"] = False\n",
    "    \n",
    "    return state\n",
    "\n",
    "# =============================================================================\n",
    "# WORKFLOW DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "# Create the workflow graph\n",
    "workflow = StateGraph(PromptOptimizationState)\n",
    "\n",
    "# Add nodes (agents)\n",
    "workflow.add_node(\"dataset_generator\", agent1_dataset_generator)\n",
    "workflow.add_node(\"prompt_executor\", agent2_prompt_executor)\n",
    "workflow.add_node(\"evaluation_analyzer\", agent3_evaluation_analyzer)\n",
    "workflow.add_node(\"enhancement_validator\", agent4_enhancement_validator)\n",
    "\n",
    "# Define the flow\n",
    "workflow.set_entry_point(\"dataset_generator\")\n",
    "\n",
    "# Sequential flow for first iteration\n",
    "workflow.add_edge(\"dataset_generator\", \"prompt_executor\")\n",
    "workflow.add_edge(\"prompt_executor\", \"evaluation_analyzer\") \n",
    "workflow.add_edge(\"evaluation_analyzer\", \"enhancement_validator\")\n",
    "\n",
    "# Conditional flow after validation\n",
    "def should_continue(state: PromptOptimizationState) -> str:\n",
    "    \"\"\"Decide whether to continue optimization or end\"\"\"\n",
    "    should_continue_opt = state.get(\"should_continue_optimizing\", False)\n",
    "    if should_continue_opt and state.get(\"iteration\", 1) <= state.get(\"max_iterations\", 5):\n",
    "        return \"prompt_executor\"  # Continue with another iteration\n",
    "    else:\n",
    "        return END  # Finalize\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"enhancement_validator\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"prompt_executor\": \"prompt_executor\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"✅ LangGraph workflow compiled successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54accd44-937f-4335-bd22-218ea783cef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_prompt = \"\"\"\n",
    "You are a helpful assistant that writes creative product descriptions for e-commerce. \n",
    "Given a product name and basic features, write an engaging product description that highlights key benefits and appeals to customers.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = '''## Context:\n",
    "    Evaluate the relevance and necessity of all the provided medical care in relation to each item in the ICD and surgery descriptions list using your professional medical knowledge.\n",
    "   \n",
    "    ## Instruction:\n",
    "    For each item in the ICD and surgery descriptions list, determine:\n",
    "    If the medical care is necessary for diagnostic or examination purposes.\n",
    "    If the medical care is effective for the disease.\n",
    "    If the benefits of medical care outweigh any risks.\n",
    "    If the medical care is a standard practice for the diagnosis.\n",
    "    If the medical care is essential for the disease and not for cosmetic/lifestyle purposes.\n",
    "    If the medical care indirectly treats the disease.\n",
    "    Medical care that is follow-up visits, repeat visits, repeat consultation, total amount, GST information, subsidies or discounts all conclude as 'yes'. Especially for GST, their descriptions may come in forms such as 'GST - ADD GST', 'GST - LESS GST'; for any descriptions that resemble these, conclude as 'yes'.\n",
    "    Considering the interactions between drugs, some drugs and treatments may not be designed for the patient's diagnoses and surgeries, but rather to counteract the side effects caused by other drugs. this scenario also needs to conclude as 'yes'\n",
    "    Analyse the exclusion details and verify if the treatment is relevant to the provided exclusion. if the treatment is relevant to the exclusion, conclude as 'no'. 'explanation' should be provided as the treatment is relevant to the exclusion list.\n",
    "    Conversely, consider the treatment medically unnecessary if it is ineffective, has safer alternatives, is discouraged by guidelines, or if risks outweigh benefits.\n",
    "   \n",
    "    ## Input: Given Medical Care: {treatments}.\n",
    "    ## ICD and Surgery Description list: {icd_surgery_description}.\n",
    "    ## Exclusion details: {exclusions}.\n",
    "   \n",
    "    ### Question: are all the given medical care at least a relevant testing or treatment for one of the items in the ICD and surgery descriptions list.\n",
    "   \n",
    "    ### Conclusion:\n",
    "    1. Explain the given result as a doctor.\n",
    "    2. Given different patient profiles and medical histories, provide a probability score without an explanation based on your analysis for the Yes/No binary output you provided to indicate the likelihood of this claim getting approved.\n",
    "   \n",
    "    ### Response: MUST Provide the output in json format with a key \"conclusion\" stating yes/no of the relevance, a key \"probability score\" providing a probability score (0-1 scale) to indicate the likelihood of this claim getting approved with 2 decimal places, and another key \"explanation\" stating the explanation as a doctor. Ensure that the conclusion is \"yes\" only if the probability score is at least 0.5. Ensure that the explanation is given in complete sentences without any truncations.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52b40054-d9aa-4ec4-9991-92346f807aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Prompt Optimization Process...\n",
      "📝 Original Prompt: ## Context:\n",
      "    Evaluate the relevance and necessity of all the provided medical care in relation to each item in the ICD and surgery descriptions list using your professional medical knowledge.\n",
      "   \n",
      "    ## Instruction:\n",
      "    For each item in the ICD and surgery descriptions list, determine:\n",
      "    If the medical care is necessary for diagnostic or examination purposes.\n",
      "    If the medical care is effective for the disease.\n",
      "    If the benefits of medical care outweigh any risks.\n",
      "    If the medical care is a standard practice for the diagnosis.\n",
      "    If the medical care is essential for the disease and not for cosmetic/lifestyle purposes.\n",
      "    If the medical care indirectly treats the disease.\n",
      "    Medical care that is follow-up visits, repeat visits, repeat consultation, total amount, GST information, subsidies or discounts all conclude as 'yes'. Especially for GST, their descriptions may come in forms such as 'GST - ADD GST', 'GST - LESS GST'; for any descriptions that resemble these, conclude as 'yes'.\n",
      "    Considering the interactions between drugs, some drugs and treatments may not be designed for the patient's diagnoses and surgeries, but rather to counteract the side effects caused by other drugs. this scenario also needs to conclude as 'yes'\n",
      "    Analyse the exclusion details and verify if the treatment is relevant to the provided exclusion. if the treatment is relevant to the exclusion, conclude as 'no'. 'explanation' should be provided as the treatment is relevant to the exclusion list.\n",
      "    Conversely, consider the treatment medically unnecessary if it is ineffective, has safer alternatives, is discouraged by guidelines, or if risks outweigh benefits.\n",
      "   \n",
      "    ## Input: Given Medical Care: {treatments}.\n",
      "    ## ICD and Surgery Description list: {icd_surgery_description}.\n",
      "    ## Exclusion details: {exclusions}.\n",
      "   \n",
      "    ### Question: are all the given medical care at least a relevant testing or treatment for one of the items in the ICD and surgery descriptions list.\n",
      "   \n",
      "    ### Conclusion:\n",
      "    1. Explain the given result as a doctor.\n",
      "    2. Given different patient profiles and medical histories, provide a probability score without an explanation based on your analysis for the Yes/No binary output you provided to indicate the likelihood of this claim getting approved.\n",
      "   \n",
      "    ### Response: MUST Provide the output in json format with a key \"conclusion\" stating yes/no of the relevance, a key \"probability score\" providing a probability score (0-1 scale) to indicate the likelihood of this claim getting approved with 2 decimal places, and another key \"explanation\" stating the explanation as a doctor. Ensure that the conclusion is \"yes\" only if the probability score is at least 0.5. Ensure that the explanation is given in complete sentences without any truncations.\n",
      "    \n",
      "================================================================================\n",
      "🤖 Agent 1: Generating test dataset...\n",
      "✅ Generated 12 test cases\n",
      "💾 Dataset saved to: datasets/test_dataset_20250616_155608.json\n",
      "\n",
      "📋 Sample test cases:\n",
      "Case 1:\n",
      "  Input: {'treatments': 'Amoxicillin 500mg TID for 7 days', 'icd_surgery_description': 'Acute bacterial sinusitis (J01.90)', 'exclusions': 'Cosmetic procedures'}\n",
      "  Expected: {'conclusion': 'yes', 'probability score': 0.95, 'explanation': 'Amoxicillin is a standard antibiotic treatment for acute bacterial sinusitis. It is effective in resolving the infection and alleviating symptoms. The benefits of treatment outweigh the risks of potential side effects like nausea or diarrhea. It is essential for treating the infection and not related to cosmetic or lifestyle purposes. This treatment is directly treating the disease.'}\n",
      "\n",
      "Case 2:\n",
      "  Input: {'treatments': 'Rhinoplasty', 'icd_surgery_description': 'Deviated septum (J34.2)', 'exclusions': 'Cosmetic procedures'}\n",
      "  Expected: {'conclusion': 'no', 'probability score': 0.2, 'explanation': 'While rhinoplasty can address a deviated septum, it is primarily considered a cosmetic procedure and is often excluded from coverage. The treatment is relevant to the exclusion list, as rhinoplasty is considered a cosmetic procedure.'}\n",
      "\n",
      "Case 3:\n",
      "  Input: {'treatments': 'Physical therapy sessions', 'icd_surgery_description': 'Lumbar spinal stenosis (M48.06)', 'exclusions': 'Experimental treatments'}\n",
      "  Expected: {'conclusion': 'yes', 'probability score': 0.85, 'explanation': \"Physical therapy is a standard and often necessary component in managing lumbar spinal stenosis. It helps improve mobility, reduce pain, and strengthen supporting muscles. The benefits typically outweigh the risks, and it's not for cosmetic purposes. The treatment indirectly treats the disease by managing the symptoms and improving function.\"}\n",
      "\n",
      "🤖 Agent 2: Executing prompt against test dataset...\n",
      "Processing test case 1/12... ✅\n",
      "Processing test case 2/12... ✅\n",
      "Processing test case 3/12... ✅\n",
      "Processing test case 4/12... ✅\n",
      "Processing test case 5/12... ✅\n",
      "Processing test case 6/12... ✅\n",
      "Processing test case 7/12... ✅\n",
      "Processing test case 8/12... ✅\n",
      "Processing test case 9/12... ✅\n",
      "Processing test case 10/12... ✅\n",
      "Processing test case 11/12... ✅\n",
      "Processing test case 12/12... ✅\n",
      "✅ Completed execution on 12 test cases\n",
      "🤖 Agent 3: Analyzing results and generating recommendations...\n",
      "The `input_variables` parameter is empty. Only the `response` column is used for computing this model-based metric.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-9a418483-e168-44a3-b639-c98ac4ce2893\" href=\"#view-view-vertex-resource-9a418483-e168-44a3-b639-c98ac4ce2893\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-9a418483-e168-44a3-b639-c98ac4ce2893');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/255766800726/locations/us-central1/metadataStores/default/contexts/prompt-optimization-experiment-861fcc11-339a-4613-91b6-5700a6611203 to Experiment: prompt-optimization-experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-212bca21-5634-4793-9052-8020c36ff1c0\" href=\"#view-view-vertex-resource-212bca21-5634-4793-9052-8020c36ff1c0\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-212bca21-5634-4793-9052-8020c36ff1c0');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-861fcc11-339a-4613-91b6-5700a6611203?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-861fcc11-339a-4613-91b6-5700a6611203?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 12 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 12 metric requests are successfully computed.\n",
      "Evaluation Took:1.8443141779862344 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-4985401e-ba2d-4d43-a9ac-658776168f46\" href=\"#view-view-vertex-resource-4985401e-ba2d-4d43-a9ac-658776168f46\">\n",
       "          <span class=\"material-icons view-vertex-icon\">bar_chart</span>\n",
       "          <span>View evaluation results</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-4985401e-ba2d-4d43-a9ac-658776168f46');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation');\n",
       "              } else {\n",
       "                window.open('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation completed\n",
      "📊 row_count: 12\n",
      "prompt_quality/mean: 5.0\n",
      "prompt_quality/std: 0.0\n",
      "Success Rate: 100.0% (12/12)\n",
      "📝 Recommendations generated\n",
      "🤖 Agent 4: Validating enhancements and finalizing prompt...\n",
      "📊 Current iteration: 1/5\n",
      "📈 Current success rate: 100.0%\n",
      "✅ Enhancement approved - Moving to iteration 2\n",
      "💡 Reasoning: Continuing optimization - minimum 3 iterations required (currently at 1)\n",
      "🔄 Enhanced prompt created\n",
      "🤖 Agent 2: Executing prompt against test dataset...\n",
      "Processing test case 1/12... ✅\n",
      "Processing test case 2/12... ✅\n",
      "Processing test case 3/12... ✅\n",
      "Processing test case 4/12... ✅\n",
      "Processing test case 5/12... ✅\n",
      "Processing test case 6/12... ✅\n",
      "Processing test case 7/12... ✅\n",
      "Processing test case 8/12... ✅\n",
      "Processing test case 9/12... ✅\n",
      "Processing test case 10/12... ✅\n",
      "Processing test case 11/12... ✅\n",
      "Processing test case 12/12... ✅\n",
      "✅ Completed execution on 12 test cases\n",
      "🤖 Agent 3: Analyzing results and generating recommendations...\n",
      "The `input_variables` parameter is empty. Only the `response` column is used for computing this model-based metric.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-5f5a362a-8641-43da-94e0-2f1661dc896c\" href=\"#view-view-vertex-resource-5f5a362a-8641-43da-94e0-2f1661dc896c\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-5f5a362a-8641-43da-94e0-2f1661dc896c');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/255766800726/locations/us-central1/metadataStores/default/contexts/prompt-optimization-experiment-fe2031c0-1170-4ffd-bd3f-9ea75944a975 to Experiment: prompt-optimization-experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-3a6d1eae-026c-43f8-a869-2ae17fa76707\" href=\"#view-view-vertex-resource-3a6d1eae-026c-43f8-a869-2ae17fa76707\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-3a6d1eae-026c-43f8-a869-2ae17fa76707');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-fe2031c0-1170-4ffd-bd3f-9ea75944a975?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-fe2031c0-1170-4ffd-bd3f-9ea75944a975?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 12 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 12 metric requests are successfully computed.\n",
      "Evaluation Took:1.8719776221551 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-41af7b82-7a1f-4ab2-bd74-6ca8be08cc37\" href=\"#view-view-vertex-resource-41af7b82-7a1f-4ab2-bd74-6ca8be08cc37\">\n",
       "          <span class=\"material-icons view-vertex-icon\">bar_chart</span>\n",
       "          <span>View evaluation results</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-41af7b82-7a1f-4ab2-bd74-6ca8be08cc37');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation');\n",
       "              } else {\n",
       "                window.open('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation completed\n",
      "📊 row_count: 12\n",
      "prompt_quality/mean: 5.0\n",
      "prompt_quality/std: 0.0\n",
      "Success Rate: 100.0% (12/12)\n",
      "📝 Recommendations generated\n",
      "🤖 Agent 4: Validating enhancements and finalizing prompt...\n",
      "📊 Current iteration: 2/5\n",
      "📈 Current success rate: 100.0%\n",
      "✅ Enhancement approved - Moving to iteration 3\n",
      "💡 Reasoning: Continuing optimization - minimum 3 iterations required (currently at 2)\n",
      "🔄 Enhanced prompt created\n",
      "🤖 Agent 2: Executing prompt against test dataset...\n",
      "Processing test case 1/12... ✅\n",
      "Processing test case 2/12... ✅\n",
      "Processing test case 3/12... ✅\n",
      "Processing test case 4/12... ✅\n",
      "Processing test case 5/12... ✅\n",
      "Processing test case 6/12... ✅\n",
      "Processing test case 7/12... ✅\n",
      "Processing test case 8/12... ✅\n",
      "Processing test case 9/12... ✅\n",
      "Processing test case 10/12... ✅\n",
      "Processing test case 11/12... ✅\n",
      "Processing test case 12/12... ✅\n",
      "✅ Completed execution on 12 test cases\n",
      "🤖 Agent 3: Analyzing results and generating recommendations...\n",
      "The `input_variables` parameter is empty. Only the `response` column is used for computing this model-based metric.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-aa201ff6-a52d-426a-9288-fd4d7b37c5f5\" href=\"#view-view-vertex-resource-aa201ff6-a52d-426a-9288-fd4d7b37c5f5\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-aa201ff6-a52d-426a-9288-fd4d7b37c5f5');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/255766800726/locations/us-central1/metadataStores/default/contexts/prompt-optimization-experiment-a0a9b98f-1986-4a92-91aa-1611e4722270 to Experiment: prompt-optimization-experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-c6ceb053-9df2-483a-b0de-324c5f25a93b\" href=\"#view-view-vertex-resource-c6ceb053-9df2-483a-b0de-324c5f25a93b\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-c6ceb053-9df2-483a-b0de-324c5f25a93b');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-a0a9b98f-1986-4a92-91aa-1611e4722270?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-a0a9b98f-1986-4a92-91aa-1611e4722270?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 12 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 12 metric requests are successfully computed.\n",
      "Evaluation Took:2.093508977908641 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-cba4d606-def8-4704-b925-f3602a1a7b7a\" href=\"#view-view-vertex-resource-cba4d606-def8-4704-b925-f3602a1a7b7a\">\n",
       "          <span class=\"material-icons view-vertex-icon\">bar_chart</span>\n",
       "          <span>View evaluation results</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-cba4d606-def8-4704-b925-f3602a1a7b7a');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation');\n",
       "              } else {\n",
       "                window.open('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation completed\n",
      "📊 row_count: 12\n",
      "prompt_quality/mean: 5.0\n",
      "prompt_quality/std: 0.0\n",
      "Success Rate: 100.0% (12/12)\n",
      "📝 Recommendations generated\n",
      "🤖 Agent 4: Validating enhancements and finalizing prompt...\n",
      "📊 Current iteration: 3/5\n",
      "📈 Current success rate: 100.0%\n",
      "✅ Enhancement approved - Moving to iteration 4\n",
      "💡 Reasoning: Continuing optimization - exploring different approaches for better results\n",
      "🔄 Enhanced prompt created\n",
      "🤖 Agent 2: Executing prompt against test dataset...\n",
      "Processing test case 1/12... ✅\n",
      "Processing test case 2/12... ✅\n",
      "Processing test case 3/12... ✅\n",
      "Processing test case 4/12... ✅\n",
      "Processing test case 5/12... ✅\n",
      "Processing test case 6/12... ✅\n",
      "Processing test case 7/12... ✅\n",
      "Processing test case 8/12... ✅\n",
      "Processing test case 9/12... ✅\n",
      "Processing test case 10/12... ✅\n",
      "Processing test case 11/12... ✅\n",
      "Processing test case 12/12... ✅\n",
      "✅ Completed execution on 12 test cases\n",
      "🤖 Agent 3: Analyzing results and generating recommendations...\n",
      "The `input_variables` parameter is empty. Only the `response` column is used for computing this model-based metric.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-7ec16e7e-4730-467f-a0c9-19185fa48cbb\" href=\"#view-view-vertex-resource-7ec16e7e-4730-467f-a0c9-19185fa48cbb\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-7ec16e7e-4730-467f-a0c9-19185fa48cbb');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/255766800726/locations/us-central1/metadataStores/default/contexts/prompt-optimization-experiment-16f1eea4-e539-4d43-b75a-5d24746f8a60 to Experiment: prompt-optimization-experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-b7a4848f-6041-47a2-a3e8-3775592ad74c\" href=\"#view-view-vertex-resource-b7a4848f-6041-47a2-a3e8-3775592ad74c\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-b7a4848f-6041-47a2-a3e8-3775592ad74c');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-16f1eea4-e539-4d43-b75a-5d24746f8a60?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-16f1eea4-e539-4d43-b75a-5d24746f8a60?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 12 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 12 metric requests are successfully computed.\n",
      "Evaluation Took:1.9122355808503926 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-a8dc8ba5-6a20-480d-9890-036726889e2d\" href=\"#view-view-vertex-resource-a8dc8ba5-6a20-480d-9890-036726889e2d\">\n",
       "          <span class=\"material-icons view-vertex-icon\">bar_chart</span>\n",
       "          <span>View evaluation results</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-a8dc8ba5-6a20-480d-9890-036726889e2d');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation');\n",
       "              } else {\n",
       "                window.open('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation completed\n",
      "📊 row_count: 12\n",
      "prompt_quality/mean: 4.666666666666667\n",
      "prompt_quality/std: 0.49236596391733095\n",
      "Success Rate: 100.0% (12/12)\n",
      "📝 Recommendations generated\n",
      "🤖 Agent 4: Validating enhancements and finalizing prompt...\n",
      "📊 Current iteration: 4/5\n",
      "📈 Current success rate: 100.0%\n",
      "✅ Enhancement approved - Moving to iteration 5\n",
      "💡 Reasoning: Continuing optimization - exploring different approaches for better results\n",
      "🔄 Enhanced prompt created\n",
      "🤖 Agent 2: Executing prompt against test dataset...\n",
      "Processing test case 1/12... ✅\n",
      "Processing test case 2/12... ✅\n",
      "Processing test case 3/12... ✅\n",
      "Processing test case 4/12... ✅\n",
      "Processing test case 5/12... ✅\n",
      "Processing test case 6/12... ✅\n",
      "Processing test case 7/12... ✅\n",
      "Processing test case 8/12... ✅\n",
      "Processing test case 9/12... ✅\n",
      "Processing test case 10/12... ✅\n",
      "Processing test case 11/12... ✅\n",
      "Processing test case 12/12... ✅\n",
      "✅ Completed execution on 12 test cases\n",
      "🤖 Agent 3: Analyzing results and generating recommendations...\n",
      "The `input_variables` parameter is empty. Only the `response` column is used for computing this model-based metric.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-22b2e642-9380-4d7e-8a8e-5f2b490ba895\" href=\"#view-view-vertex-resource-22b2e642-9380-4d7e-8a8e-5f2b490ba895\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-22b2e642-9380-4d7e-8a8e-5f2b490ba895');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/255766800726/locations/us-central1/metadataStores/default/contexts/prompt-optimization-experiment-48b3d289-e654-48c4-984a-4ec43b3e3597 to Experiment: prompt-optimization-experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-2fd019d8-abaf-485c-9dd8-3f5259f84fd3\" href=\"#view-view-vertex-resource-2fd019d8-abaf-485c-9dd8-3f5259f84fd3\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-2fd019d8-abaf-485c-9dd8-3f5259f84fd3');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-48b3d289-e654-48c4-984a-4ec43b3e3597?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-48b3d289-e654-48c4-984a-4ec43b3e3597?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 12 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 12 metric requests are successfully computed.\n",
      "Evaluation Took:1.883266766089946 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-528b53ae-8c8f-4c7c-85cf-e3aacaa86ef9\" href=\"#view-view-vertex-resource-528b53ae-8c8f-4c7c-85cf-e3aacaa86ef9\">\n",
       "          <span class=\"material-icons view-vertex-icon\">bar_chart</span>\n",
       "          <span>View evaluation results</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-528b53ae-8c8f-4c7c-85cf-e3aacaa86ef9');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation');\n",
       "              } else {\n",
       "                window.open('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation completed\n",
      "📊 row_count: 12\n",
      "prompt_quality/mean: 5.0\n",
      "prompt_quality/std: 0.0\n",
      "Success Rate: 100.0% (12/12)\n",
      "📝 Recommendations generated\n",
      "🤖 Agent 4: Validating enhancements and finalizing prompt...\n",
      "📊 Current iteration: 5/5\n",
      "📈 Current success rate: 100.0%\n",
      "🎯 Finalizing prompt - analyzing all iterations...\n",
      "✅ Final prompt selected\n",
      "💡 Reasoning: Reached maximum iterations (5) - time to select best prompt\n",
      "🏆 Best iteration was #1 with 100.0% success rate\n",
      "💾 Comprehensive results saved to readme_prompt.md\n",
      "\n",
      "================================================================================\n",
      "🎉 OPTIMIZATION COMPLETE!\n",
      "================================================================================\n",
      "📊 Final Results:\n",
      "   - Total Iterations: 5\n",
      "   - Test Cases: 12\n",
      "   - Success Rate: 12/12\n",
      "\n",
      "📝 Original Prompt:\n",
      "   ## Context:\n",
      "    Evaluate the relevance and necessity of all the provided medical care in relation to each item in the ICD and surgery descriptions list using your professional medical knowledge.\n",
      "   \n",
      "    ## Instruction:\n",
      "    For each item in the ICD and surgery descriptions list, determine:\n",
      "    If the medical care is necessary for diagnostic or examination purposes.\n",
      "    If the medical care is effective for the disease.\n",
      "    If the benefits of medical care outweigh any risks.\n",
      "    If the medical care is a standard practice for the diagnosis.\n",
      "    If the medical care is essential for the disease and not for cosmetic/lifestyle purposes.\n",
      "    If the medical care indirectly treats the disease.\n",
      "    Medical care that is follow-up visits, repeat visits, repeat consultation, total amount, GST information, subsidies or discounts all conclude as 'yes'. Especially for GST, their descriptions may come in forms such as 'GST - ADD GST', 'GST - LESS GST'; for any descriptions that resemble these, conclude as 'yes'.\n",
      "    Considering the interactions between drugs, some drugs and treatments may not be designed for the patient's diagnoses and surgeries, but rather to counteract the side effects caused by other drugs. this scenario also needs to conclude as 'yes'\n",
      "    Analyse the exclusion details and verify if the treatment is relevant to the provided exclusion. if the treatment is relevant to the exclusion, conclude as 'no'. 'explanation' should be provided as the treatment is relevant to the exclusion list.\n",
      "    Conversely, consider the treatment medically unnecessary if it is ineffective, has safer alternatives, is discouraged by guidelines, or if risks outweigh benefits.\n",
      "   \n",
      "    ## Input: Given Medical Care: {treatments}.\n",
      "    ## ICD and Surgery Description list: {icd_surgery_description}.\n",
      "    ## Exclusion details: {exclusions}.\n",
      "   \n",
      "    ### Question: are all the given medical care at least a relevant testing or treatment for one of the items in the ICD and surgery descriptions list.\n",
      "   \n",
      "    ### Conclusion:\n",
      "    1. Explain the given result as a doctor.\n",
      "    2. Given different patient profiles and medical histories, provide a probability score without an explanation based on your analysis for the Yes/No binary output you provided to indicate the likelihood of this claim getting approved.\n",
      "   \n",
      "    ### Response: MUST Provide the output in json format with a key \"conclusion\" stating yes/no of the relevance, a key \"probability score\" providing a probability score (0-1 scale) to indicate the likelihood of this claim getting approved with 2 decimal places, and another key \"explanation\" stating the explanation as a doctor. Ensure that the conclusion is \"yes\" only if the probability score is at least 0.5. Ensure that the explanation is given in complete sentences without any truncations.\n",
      "    \n",
      "\n",
      "🎯 Final Prompt:\n",
      "   ```\n",
      "## Context:\n",
      "You are a highly experienced medical claim reviewer specializing in evaluating the medical necessity and appropriateness of treatments for insurance claim approval. Your task is to determine if the provided medical care is *both* relevant *and* medically necessary for the diagnoses and procedures listed. Base your evaluation on standard medical practices, potential exclusions defined in the insurance policy, the presence or absence of supporting documentation, and the overall likelihood of claim approval. You must provide a clear, concise justification for your decision.\n",
      "\n",
      "## Instruction:\n",
      "\n",
      "### Relevance Criteria:\n",
      "For each item in the ICD and surgery descriptions list, rigorously evaluate whether the given medical care meets at least one of the following criteria:\n",
      "\n",
      "1. **Diagnostic Necessity:** Is the medical care required for accurate diagnosis or examination of the patient's condition?\n",
      "2. **Treatment Effectiveness:** Is the medical care a recognized and effective treatment for the diagnosed disease or condition? Base your assessment on established medical guidelines and peer-reviewed literature.\n",
      "3. **Risk-Benefit Ratio:** Do the potential benefits of the medical care demonstrably outweigh any associated risks to the patient, considering their specific medical history and current condition?\n",
      "4. **Standard of Care:** Is the medical care considered a standard or generally accepted practice for the diagnosis and treatment of the condition in question? This does *not* necessarily mean it's the *only* acceptable treatment, but rather that it's a recognized and appropriate option.\n",
      "5. **Essential vs. Cosmetic/Lifestyle:** Is the medical care primarily essential for treating the disease or condition, rather than being solely for cosmetic enhancement or lifestyle improvement? Consider procedures with mixed indications carefully, weighing the medical benefits.\n",
      "6. **Direct or Indirect Treatment:** Does the medical care directly address the underlying cause of the disease, or does it indirectly treat the symptoms or complications (e.g., pain management for chronic pain, medication to counteract side effects of other treatments)?\n",
      "\n",
      "While the following criteria are provided as guidance, you should also leverage your own medical knowledge and understanding of standard practices to evaluate the claim.\n",
      "\n",
      "### Special Cases and Considerations:\n",
      "\n",
      "*   **Administrative Items:** Medical care related solely to follow-up visits, repeat visits/consultations for administrative purposes, total amounts, GST information (e.g., 'GST - ADD GST', 'GST - LESS GST'), subsidies, or discounts should ALWAYS be considered relevant (conclusion = \"yes\"). These are necessary for processing the claim.\n",
      "*   **Drug Interactions & Side Effects:** Carefully consider potential interactions between drugs and treatments. Some medications may be prescribed specifically to manage or counteract side effects caused by other drugs, even if they do not directly treat the primary diagnosis. These are considered relevant.\n",
      "*   **Off-Label Use:** If a treatment is being used \\\"off-label\\\" (i.e., for a condition not specifically approved by regulatory agencies), research and justify its use based on credible medical literature. The explanation must address the off-label nature and the supporting evidence.\n",
      "\n",
      "### Exclusion Handling:\n",
      "\n",
      "*   **Strictly Adhere to Exclusions:** Analyze the provided exclusion details with utmost care. If the treatment is CLEARLY and DIRECTLY relevant to a stated exclusion, the conclusion MUST be \\\"no.\\\" The 'explanation' should explicitly state the applicable exclusion and why the treatment falls under it. If the relationship between the treatment and the exclusion is unclear or arguable, give a lower probability score and explore potential justifications for the treatment's medical necessity. Consider whether the treatment addresses a separate, non-excluded condition, or whether the exclusion is being applied too broadly.\n",
      "*   **Justifiable Exceptions:** In rare cases, a treatment that *appears* to be excluded might be justifiable due to unique patient circumstances or evolving medical evidence. **However, such exceptions require exceptionally strong and clearly documented justification based on medical literature and the specific patient's case.** Increase the probability score *only* if this justification is compelling and highly persuasive.\n",
      "\n",
      "### Probability Score Guidance:\n",
      "\n",
      "The \\\"probability score\\\" represents the likelihood of the claim being approved by an insurance provider. Consider the following factors when assigning this score: When in doubt, or when the medical necessity is not unequivocally established based on the provided information, lower the probability score.\n",
      "\n",
      "*   **Medical Necessity:** Higher scores for treatments that are clearly medically necessary and aligned with standard practices.\n",
      "*   **Supporting Documentation:** The presence of strong supporting documentation (e.g., physician notes, lab results, imaging reports) significantly increases the probability score. In the *absence* of explicit mention of such documentation, consider the *likelihood* that it would typically exist for the given treatment and diagnosis. Common and standard treatments (e.g., antibiotics for bacterial infections, physical therapy for spinal stenosis) often have an implicit expectation of supporting documentation. If such documentation is highly likely to exist in standard practice, the probability score should not be drastically reduced. If there is reason to believe that supporting documentation *would not* typically exist, or if the treatment is unusual or experimental, then a lower probability score is warranted.\n",
      "\n",
      "    For instance, if the treatment is 'Insulin for Type 1 Diabetes', it is highly likely that the patient has been diagnosed with Type 1 Diabetes, and lab results would exist to support this diagnosis. Therefore, even without explicit mention of documentation, the probability score should remain high (0.8 - 0.9) due to the implicit evidence.\n",
      "\n",
      "*   **Exclusion Relevance:** Lower scores for treatments that are potentially subject to exclusions.\n",
      "*   **Off-Label Use:** Lower scores for off-label uses, unless strongly supported by medical literature.\n",
      "*   **Pre-authorization Requirements:** Consider whether the treatment requires pre-authorization and if the patient has obtained it.\n",
      "*   **Insurance Policy:** While you don't have access to the specific policy, consider general insurance practices regarding similar claims.\n",
      "\n",
      "*   **High Probability (0.8 - 1.0):** Treatment is clearly medically necessary, aligned with standard practices, no relevant exclusions, and supporting documentation is either explicitly mentioned or highly likely to exist. Do not hesitate to assign a probability score in the 0.8-1.0 range if these conditions are met. Provide a strong justification for these higher scores.\n",
      "*   **Moderate Probability (0.5 - 0.79):** Treatment is generally accepted for the condition, but there might be minor concerns (e.g., lack of specific documentation, potential for off-label use with some supporting evidence, or a slightly ambiguous exclusion). Claim approval is reasonably likely.\n",
      "*   **Low Probability (0.0 - 0.49):** Significant concerns exist, such as a clear exclusion, lack of medical necessity, strong suspicion of cosmetic or lifestyle treatment, or very weak/absent supporting documentation. Claim approval is unlikely.\n",
      "\n",
      "### Output Format:\n",
      "\n",
      "**MUST** Provide the output in JSON format with the following keys:\n",
      "\n",
      "*   `conclusion`: A string stating \\\"yes\\\" or \\\"no\\\" to indicate the relevance and medical necessity of the medical care.\n",
      "*   `probability score`: A numerical value (0-1 scale) representing the estimated likelihood of claim approval, with 2 decimal places. This score should reflect the overall strength of the medical justification, potential exclusions, and general insurance practices.\n",
      "*   `explanation`: A comprehensive and detailed explanation, written as a medical claim reviewer would explain to a colleague or a healthcare provider appealing a denied claim. The explanation *must* explicitly state which of the Relevance Criteria are met (e.g., 'This treatment meets the Treatment Effectiveness criterion because...' ). A suggested structure for the explanation is: 1) Briefly describe the treatment and its purpose. 2) Explain how the treatment relates to the diagnosis/procedure, referencing specific Relevance Criteria. 3) Address any potential exclusions and explain why they do or do not apply. 4) State the overall medical necessity. 5) Conclude by justifying the assigned probability score. Justify the 'conclusion' and the assigned 'probability score' with specific references to the relevance criteria, special cases, exclusion handling, and probability score guidance. Ensure explanations are in complete sentences without any truncations. **Begin the explanation with a bolded sentence stating the conclusion.**\n",
      "\n",
      "**IMPORTANT:** The `conclusion` MUST be 'yes' **IF AND ONLY IF** the `probability score` is at least 0.5. If the probability score is below 0.5, the conclusion MUST be 'no'.\n",
      "**IMPORTANT:** The `probability score` MUST NOT be higher than 0.75 if there is no explicit mention of the existence of supporting documentations (e.g., physician notes, lab results, imaging reports) in the explanation.\n",
      "\n",
      "## Input: Given Medical Care: {treatments}.\n",
      "## ICD and Surgery Description list: {icd_surgery_description}.\n",
      "## Exclusion details: {exclusions}.\n",
      "\n",
      "### Question: Based on the provided information, is the given medical care a relevant and medically necessary testing or treatment for at least one of the items in the ICD and surgery descriptions list, considering potential exclusions and the likelihood of claim approval?\n",
      "```\n",
      "\n",
      "💾 Detailed results saved to: readme_prompt.md\n",
      "\n",
      "✅ Process completed!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTION EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "print(\"🚀 Starting Prompt Optimization Process...\")\n",
    "print(f\"📝 Original Prompt: {user_prompt}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize state\n",
    "initial_state = {\n",
    "    \"original_prompt\": user_prompt,\n",
    "    \"current_prompt\": user_prompt,\n",
    "    \"test_dataset\": [],\n",
    "    \"agent2_results\": [],\n",
    "    \"evaluation_results\": {},\n",
    "    \"enhancement_recommendations\": \"\",\n",
    "    \"enhancement_makes_sense\": False,\n",
    "    \"final_prompt\": \"\",\n",
    "    \"iteration\": 1,\n",
    "    \"max_iterations\": 5,  # Increased to 5\n",
    "    \"iteration_history\": [],\n",
    "    \"should_continue_optimizing\": True\n",
    "}\n",
    "\n",
    "# Run the workflow\n",
    "try:\n",
    "    final_state = app.invoke(initial_state)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎉 OPTIMIZATION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"📊 Final Results:\")\n",
    "    print(f\"   - Total Iterations: {final_state.get('iteration', 1)}\")\n",
    "    print(f\"   - Test Cases: {len(final_state.get('test_dataset', []))}\")\n",
    "    print(f\"   - Success Rate: {final_state.get('evaluation_results', {}).get('success_cases', 0)}/{final_state.get('evaluation_results', {}).get('total_cases', 0)}\")\n",
    "    \n",
    "    print(f\"\\n📝 Original Prompt:\")\n",
    "    print(f\"   {final_state['original_prompt']}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Final Prompt:\")\n",
    "    print(f\"   {final_state.get('final_prompt', final_state['current_prompt'])}\")\n",
    "    \n",
    "    if os.path.exists(\"readme_prompt.md\"):\n",
    "        print(f\"\\n💾 Detailed results saved to: readme_prompt.md\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during execution: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n✅ Process completed!\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py312",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "conda-base-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
