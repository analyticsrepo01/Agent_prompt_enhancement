{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d4395-297a-409f-931d-22f4c8843538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d6a687-06ce-4c40-b7b4-4640f489929c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Project ID: my-project-0004-346516\n",
      "Location: us-central1\n",
      "✅ Setup completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# LangGraph Prompt Optimization System\n",
    "# This notebook creates a 4-agent system for prompt optimization using LangGraph\n",
    "\n",
    "# Installation and Setup\n",
    "%pip install --upgrade --quiet langgraph langchain-google-vertexai google-cloud-aiplatform[evaluation] pandas\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, TypedDict, Annotated\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Google Cloud Setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"us-central1\"\n",
    "EXPERIMENT_NAME = \"prompt-optimization-experiment\"\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\"\n",
    "\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Location: {LOCATION}\")\n",
    "\n",
    "# LangGraph and LangChain imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "import vertexai\n",
    "from vertexai.evaluation import EvalTask, PointwiseMetric, PointwiseMetricPromptTemplate\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-2.0-flash-001\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "print(\"✅ Setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "222b71ff-e821-4c29-aaa3-98823b05ecdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ State definition completed!\n",
      "✅ LangGraph workflow compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# STATE DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "# Define the state that will be passed between agents\n",
    "class PromptOptimizationState(TypedDict):\n",
    "    original_prompt: str\n",
    "    current_prompt: str\n",
    "    test_dataset: List[Dict[str, str]]\n",
    "    agent2_results: List[Dict[str, Any]]\n",
    "    evaluation_results: Dict[str, Any]\n",
    "    enhancement_recommendations: str\n",
    "    enhancement_makes_sense: bool\n",
    "    final_prompt: str\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "\n",
    "print(\"✅ State definition completed!\")\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT 1: DATASET GENERATOR\n",
    "# =============================================================================\n",
    "\n",
    "def agent1_dataset_generator(state: PromptOptimizationState) -> PromptOptimizationState:\n",
    "    \"\"\"\n",
    "    Agent 1: Generates a dataset of input-output pairs for testing the prompt\n",
    "    \"\"\"\n",
    "    print(\"🤖 Agent 1: Generating test dataset...\")\n",
    "    \n",
    "    original_prompt = state[\"original_prompt\"]\n",
    "    \n",
    "    dataset_generation_prompt = f\"\"\"\n",
    "    You are a dataset generation expert. Given the following prompt, generate 12 diverse input-output pairs that would be good for testing this prompt.\n",
    "    \n",
    "    Original Prompt: {original_prompt}\n",
    "    \n",
    "    Generate 12 test cases with varied inputs that would help evaluate how well this prompt performs. \n",
    "    Each test case should have:\n",
    "    - input: A realistic input scenario\n",
    "    - expected_output: What a good response should look like\n",
    "    \n",
    "    Make the test cases diverse to cover different scenarios, edge cases, and complexity levels.\n",
    "    \n",
    "    Return ONLY a valid JSON array in this format:\n",
    "    [\n",
    "        {{\"input\": \"test input 1\", \"expected_output\": \"expected response 1\"}},\n",
    "        {{\"input\": \"test input 2\", \"expected_output\": \"expected response 2\"}},\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=dataset_generation_prompt)])\n",
    "    \n",
    "    try:\n",
    "        # Extract JSON from response\n",
    "        response_text = response.content.strip()\n",
    "        if response_text.startswith(\"```json\"):\n",
    "            response_text = response_text[7:-3].strip()\n",
    "        elif response_text.startswith(\"```\"):\n",
    "            response_text = response_text[3:-3].strip()\n",
    "        \n",
    "        test_dataset = json.loads(response_text)\n",
    "        \n",
    "        # Validate dataset structure\n",
    "        if not isinstance(test_dataset, list) or len(test_dataset) != 12:\n",
    "            raise ValueError(\"Dataset should be a list of 12 items\")\n",
    "        \n",
    "        for item in test_dataset:\n",
    "            if not isinstance(item, dict) or \"input\" not in item or \"expected_output\" not in item:\n",
    "                raise ValueError(\"Each item should have 'input' and 'expected_output' keys\")\n",
    "        \n",
    "        state[\"test_dataset\"] = test_dataset\n",
    "        print(f\"✅ Generated {len(test_dataset)} test cases\")\n",
    "        \n",
    "        # Save dataset to file\n",
    "        import os\n",
    "        import datetime\n",
    "        \n",
    "        # Create datasets folder if it doesn't exist\n",
    "        os.makedirs(\"datasets\", exist_ok=True)\n",
    "        \n",
    "        # Generate filename with timestamp to avoid overwriting\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"datasets/test_dataset_{timestamp}.json\"\n",
    "        \n",
    "        # Check if file exists and create versioned filename\n",
    "        counter = 1\n",
    "        original_filename = filename\n",
    "        while os.path.exists(filename):\n",
    "            base_name = original_filename.replace(\".json\", \"\")\n",
    "            filename = f\"{base_name}_v{counter}.json\"\n",
    "            counter += 1\n",
    "        \n",
    "        # Save the dataset\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(test_dataset, f, indent=2)\n",
    "        \n",
    "        print(f\"💾 Dataset saved to: {filename}\")\n",
    "        \n",
    "        # Display sample test cases\n",
    "        print(\"\\n📋 Sample test cases:\")\n",
    "        for i, case in enumerate(test_dataset[:3]):\n",
    "            print(f\"Case {i+1}:\")\n",
    "            input_text = case['input'] if len(case['input']) <= 100 else case['input'][:100] + \"...\"\n",
    "            expected_text = case['expected_output'] if len(case['expected_output']) <= 100 else case['expected_output'][:100] + \"...\"\n",
    "            print(f\"  Input: {input_text}\")\n",
    "            print(f\"  Expected: {expected_text}\")\n",
    "            print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating dataset: {e}\")\n",
    "        # Fallback: create a simple dataset\n",
    "        state[\"test_dataset\"] = [\n",
    "            {\"input\": f\"Test input {i+1}\", \"expected_output\": f\"Expected output {i+1}\"}\n",
    "            for i in range(12)\n",
    "        ]\n",
    "    \n",
    "    return state\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT 2: PROMPT EXECUTOR\n",
    "# =============================================================================\n",
    "\n",
    "def agent2_prompt_executor(state: PromptOptimizationState) -> PromptOptimizationState:\n",
    "    \"\"\"\n",
    "    Agent 2: Executes the current prompt against all test cases and collects results\n",
    "    \"\"\"\n",
    "    print(\"🤖 Agent 2: Executing prompt against test dataset...\")\n",
    "    \n",
    "    current_prompt = state[\"current_prompt\"]\n",
    "    test_dataset = state[\"test_dataset\"]\n",
    "    results = []\n",
    "    \n",
    "    for i, test_case in enumerate(test_dataset):\n",
    "        print(f\"Processing test case {i+1}/12...\", end=\" \")\n",
    "        \n",
    "        # Apply the current prompt to the test input\n",
    "        full_prompt = f\"{current_prompt}\\n\\nInput: {test_case['input']}\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke([HumanMessage(content=full_prompt)])\n",
    "            actual_output = response.content.strip()\n",
    "            \n",
    "            result = {\n",
    "                \"test_case_id\": i + 1,\n",
    "                \"input\": test_case[\"input\"],\n",
    "                \"expected_output\": test_case[\"expected_output\"],\n",
    "                \"actual_output\": actual_output,\n",
    "                \"prompt_used\": current_prompt\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(\"✅\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            result = {\n",
    "                \"test_case_id\": i + 1,\n",
    "                \"input\": test_case[\"input\"],\n",
    "                \"expected_output\": test_case[\"expected_output\"],\n",
    "                \"actual_output\": f\"ERROR: {str(e)}\",\n",
    "                \"prompt_used\": current_prompt\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    state[\"agent2_results\"] = results\n",
    "    print(f\"✅ Completed execution on {len(results)} test cases\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT 3: EVALUATION ANALYZER\n",
    "# =============================================================================\n",
    "\n",
    "def agent3_evaluation_analyzer(state: PromptOptimizationState) -> PromptOptimizationState:\n",
    "    \"\"\"\n",
    "    Agent 3: Analyzes the results using Vertex AI evaluation and provides enhancement recommendations\n",
    "    \"\"\"\n",
    "    print(\"🤖 Agent 3: Analyzing results and generating recommendations...\")\n",
    "    \n",
    "    results = state[\"agent2_results\"]\n",
    "    \n",
    "    # Create evaluation dataset\n",
    "    eval_data = []\n",
    "    for result in results:\n",
    "        eval_data.append({\n",
    "            \"input\": result[\"input\"],\n",
    "            \"expected_output\": result[\"expected_output\"],\n",
    "            \"response\": result[\"actual_output\"]  # Changed from actual_output to response\n",
    "        })\n",
    "    \n",
    "    eval_df = pd.DataFrame(eval_data)\n",
    "    \n",
    "    # Define custom evaluation metric for prompt quality\n",
    "    prompt_quality_metric = PointwiseMetric(\n",
    "        metric=\"prompt_quality\",\n",
    "        metric_prompt_template=PointwiseMetricPromptTemplate(\n",
    "            criteria={\n",
    "                \"accuracy\": \"The actual output matches the expected output in terms of correctness and completeness\",\n",
    "                \"relevance\": \"The actual output is relevant to the input and addresses the main points\",\n",
    "                \"clarity\": \"The actual output is clear, well-structured, and easy to understand\",\n",
    "                \"consistency\": \"The output style and format are consistent with expectations\"\n",
    "            },\n",
    "            rating_rubric={\n",
    "                \"5\": \"Excellent: Meets all criteria exceptionally well\",\n",
    "                \"4\": \"Good: Meets most criteria well with minor issues\",\n",
    "                \"3\": \"Average: Meets some criteria but has notable gaps\",\n",
    "                \"2\": \"Poor: Falls short on most criteria\",\n",
    "                \"1\": \"Very Poor: Fails to meet criteria\"\n",
    "            },\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Run evaluation\n",
    "        eval_task = EvalTask(\n",
    "            dataset=eval_df,\n",
    "            metrics=[prompt_quality_metric],\n",
    "            experiment=EXPERIMENT_NAME\n",
    "        )\n",
    "        \n",
    "        eval_result = eval_task.evaluate()\n",
    "        \n",
    "        # Extract evaluation scores\n",
    "        scores = []\n",
    "        if hasattr(eval_result, 'summary_metrics'):\n",
    "            for metric_name, metric_value in eval_result.summary_metrics.items():\n",
    "                scores.append(f\"{metric_name}: {metric_value}\")\n",
    "        \n",
    "        evaluation_summary = \"\\n\".join(scores) if scores else \"Evaluation completed\"\n",
    "        \n",
    "        # Calculate simple metrics as fallback\n",
    "        total_cases = len(results)\n",
    "        success_cases = sum(1 for r in results if \"ERROR\" not in r[\"actual_output\"])\n",
    "        success_rate = (success_cases / total_cases) * 100 if total_cases > 0 else 0\n",
    "        \n",
    "        evaluation_summary += f\"\\nSuccess Rate: {success_rate:.1f}% ({success_cases}/{total_cases})\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Evaluation error: {e}\")\n",
    "        # Fallback evaluation\n",
    "        total_cases = len(results)\n",
    "        success_cases = sum(1 for r in results if \"ERROR\" not in r[\"actual_output\"])\n",
    "        success_rate = (success_cases / total_cases) * 100 if total_cases > 0 else 0\n",
    "        evaluation_summary = f\"Basic Evaluation - Success Rate: {success_rate:.1f}% ({success_cases}/{total_cases})\"\n",
    "    \n",
    "    # Generate enhancement recommendations using LLM\n",
    "    analysis_prompt = f\"\"\"\n",
    "    You are a prompt engineering expert. Analyze the following test results and provide specific recommendations to improve the prompt.\n",
    "    \n",
    "    Original Prompt: {state[\"current_prompt\"]}\n",
    "    \n",
    "    Evaluation Summary: {evaluation_summary}\n",
    "    \n",
    "    Sample Results:\n",
    "    {json.dumps(results[:3], indent=2)}\n",
    "    \n",
    "    Based on this analysis, provide specific, actionable recommendations to enhance the prompt. \n",
    "    Focus on:\n",
    "    1. What patterns of errors or suboptimal responses do you see?\n",
    "    2. How can the prompt be made clearer or more specific?\n",
    "    3. What instructions or examples should be added?\n",
    "    4. What formatting or structure improvements are needed?\n",
    "    \n",
    "    Provide your recommendations in a clear, structured format.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        analysis_response = llm.invoke([HumanMessage(content=analysis_prompt)])\n",
    "        enhancement_recommendations = analysis_response.content.strip()\n",
    "    except Exception as e:\n",
    "        enhancement_recommendations = f\"Error generating recommendations: {e}\"\n",
    "    \n",
    "    state[\"evaluation_results\"] = {\n",
    "        \"summary\": evaluation_summary,\n",
    "        \"total_cases\": len(results),\n",
    "        \"success_cases\": sum(1 for r in results if \"ERROR\" not in r[\"actual_output\"]),\n",
    "        \"detailed_results\": results\n",
    "    }\n",
    "    state[\"enhancement_recommendations\"] = enhancement_recommendations\n",
    "    \n",
    "    print(\"✅ Evaluation completed\")\n",
    "    print(f\"📊 {evaluation_summary}\")\n",
    "    print(f\"📝 Recommendations generated\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# =============================================================================\n",
    "# AGENT 4: ENHANCEMENT VALIDATOR\n",
    "# =============================================================================\n",
    "\n",
    "def agent4_enhancement_validator(state: PromptOptimizationState) -> PromptOptimizationState:\n",
    "    \"\"\"\n",
    "    Agent 4: Validates if enhancement recommendations make sense and creates final prompt\n",
    "    \"\"\"\n",
    "    print(\"🤖 Agent 4: Validating enhancements and finalizing prompt...\")\n",
    "    \n",
    "    current_prompt = state[\"current_prompt\"]\n",
    "    recommendations = state[\"enhancement_recommendations\"]\n",
    "    evaluation_results = state[\"evaluation_results\"]\n",
    "    iteration = state.get(\"iteration\", 1)\n",
    "    max_iterations = state.get(\"max_iterations\", 3)\n",
    "    \n",
    "    # Validate recommendations\n",
    "    validation_prompt = f\"\"\"\n",
    "    You are a prompt engineering validator. Review the current prompt and the proposed enhancement recommendations.\n",
    "    \n",
    "    Current Prompt: {current_prompt}\n",
    "    \n",
    "    Enhancement Recommendations: {recommendations}\n",
    "    \n",
    "    Evaluation Results: {evaluation_results.get(\"summary\", \"No summary available\")}\n",
    "    \n",
    "    Current Iteration: {iteration}/{max_iterations}\n",
    "    \n",
    "    Determine if these recommendations make sense and would improve the prompt. Consider:\n",
    "    1. Are the recommendations specific and actionable?\n",
    "    2. Do they address real issues identified in the evaluation?\n",
    "    3. Would implementing them likely improve performance?\n",
    "    4. Are they reasonable and not over-complicated?\n",
    "    \n",
    "    Respond with:\n",
    "    - \"ENHANCE\" if the recommendations should be implemented, followed by an improved version of the prompt\n",
    "    - \"FINALIZE\" if the current prompt is good enough or if we've reached max iterations\n",
    "    \n",
    "    Format your response as:\n",
    "    DECISION: [ENHANCE/FINALIZE]\n",
    "    REASONING: [Your reasoning]\n",
    "    PROMPT: [The final or enhanced prompt]\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        validation_response = llm.invoke([HumanMessage(content=validation_prompt)])\n",
    "        response_text = validation_response.content.strip()\n",
    "        \n",
    "        # Parse the response\n",
    "        lines = response_text.split('\\n')\n",
    "        decision = None\n",
    "        reasoning = \"\"\n",
    "        new_prompt = current_prompt\n",
    "        \n",
    "        current_section = None\n",
    "        for line in lines:\n",
    "            if line.startswith(\"DECISION:\"):\n",
    "                decision = line.replace(\"DECISION:\", \"\").strip()\n",
    "                current_section = \"decision\"\n",
    "            elif line.startswith(\"REASONING:\"):\n",
    "                current_section = \"reasoning\"\n",
    "                reasoning = line.replace(\"REASONING:\", \"\").strip()\n",
    "            elif line.startswith(\"PROMPT:\"):\n",
    "                current_section = \"prompt\"\n",
    "                new_prompt = line.replace(\"PROMPT:\", \"\").strip()\n",
    "            elif current_section == \"reasoning\":\n",
    "                reasoning += \" \" + line.strip()\n",
    "            elif current_section == \"prompt\":\n",
    "                new_prompt += \"\\n\" + line\n",
    "        \n",
    "        # Check if we should enhance or finalize\n",
    "        should_enhance = (decision == \"ENHANCE\" and \n",
    "                         iteration < max_iterations and \n",
    "                         evaluation_results.get(\"success_cases\", 0) < evaluation_results.get(\"total_cases\", 12) * 0.8)\n",
    "        \n",
    "        state[\"enhancement_makes_sense\"] = should_enhance\n",
    "        \n",
    "        if should_enhance:\n",
    "            state[\"current_prompt\"] = new_prompt.strip()\n",
    "            state[\"iteration\"] = iteration + 1\n",
    "            print(f\"✅ Enhancement approved - Moving to iteration {state['iteration']}\")\n",
    "            print(f\"💡 Reasoning: {reasoning}\")\n",
    "        else:\n",
    "            state[\"final_prompt\"] = new_prompt.strip()\n",
    "            print(\"✅ Finalization approved\")\n",
    "            print(f\"💡 Reasoning: {reasoning}\")\n",
    "            \n",
    "            # Save to file\n",
    "            with open(\"readme_prompt.md\", \"w\") as f:\n",
    "                f.write(\"# Final Optimized Prompt\\n\\n\")\n",
    "                f.write(f\"## Original Prompt\\n{state['original_prompt']}\\n\\n\")\n",
    "                f.write(f\"## Final Prompt\\n{state['final_prompt']}\\n\\n\")\n",
    "                f.write(f\"## Optimization Summary\\n\")\n",
    "                f.write(f\"- Iterations: {iteration}\\n\")\n",
    "                f.write(f\"- Final Success Rate: {evaluation_results.get('success_cases', 0)}/{evaluation_results.get('total_cases', 12)}\\n\")\n",
    "                f.write(f\"- Last Enhancement Reasoning: {reasoning}\\n\\n\")\n",
    "                f.write(f\"## Evaluation Results\\n{evaluation_results.get('summary', 'No summary')}\\n\")\n",
    "            \n",
    "            print(\"💾 Final prompt saved to readme_prompt.md\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in validation: {e}\")\n",
    "        state[\"enhancement_makes_sense\"] = False\n",
    "        state[\"final_prompt\"] = current_prompt\n",
    "    \n",
    "    return state\n",
    "\n",
    "# =============================================================================\n",
    "# WORKFLOW DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "# Create the workflow graph\n",
    "workflow = StateGraph(PromptOptimizationState)\n",
    "\n",
    "# Add nodes (agents)\n",
    "workflow.add_node(\"dataset_generator\", agent1_dataset_generator)\n",
    "workflow.add_node(\"prompt_executor\", agent2_prompt_executor)\n",
    "workflow.add_node(\"evaluation_analyzer\", agent3_evaluation_analyzer)\n",
    "workflow.add_node(\"enhancement_validator\", agent4_enhancement_validator)\n",
    "\n",
    "# Define the flow\n",
    "workflow.set_entry_point(\"dataset_generator\")\n",
    "\n",
    "# Sequential flow for first iteration\n",
    "workflow.add_edge(\"dataset_generator\", \"prompt_executor\")\n",
    "workflow.add_edge(\"prompt_executor\", \"evaluation_analyzer\") \n",
    "workflow.add_edge(\"evaluation_analyzer\", \"enhancement_validator\")\n",
    "\n",
    "# Conditional flow after validation\n",
    "def should_continue(state: PromptOptimizationState) -> str:\n",
    "    \"\"\"Decide whether to continue optimization or end\"\"\"\n",
    "    if state.get(\"enhancement_makes_sense\", False) and state.get(\"iteration\", 1) < state.get(\"max_iterations\", 3):\n",
    "        return \"prompt_executor\"  # Continue with another iteration\n",
    "    else:\n",
    "        return END  # Finalize\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"enhancement_validator\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"prompt_executor\": \"prompt_executor\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"✅ LangGraph workflow compiled successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54accd44-937f-4335-bd22-218ea783cef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_prompt = \"\"\"\n",
    "You are a helpful assistant that writes creative product descriptions for e-commerce. \n",
    "Given a product name and basic features, write an engaging product description that highlights key benefits and appeals to customers.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = '''## Context:\n",
    "    Evaluate the relevance and necessity of all the provided medical care in relation to each item in the ICD and surgery descriptions list using your professional medical knowledge.\n",
    "   \n",
    "    ## Instruction:\n",
    "    For each item in the ICD and surgery descriptions list, determine:\n",
    "    If the medical care is necessary for diagnostic or examination purposes.\n",
    "    If the medical care is effective for the disease.\n",
    "    If the benefits of medical care outweigh any risks.\n",
    "    If the medical care is a standard practice for the diagnosis.\n",
    "    If the medical care is essential for the disease and not for cosmetic/lifestyle purposes.\n",
    "    If the medical care indirectly treats the disease.\n",
    "    Medical care that is follow-up visits, repeat visits, repeat consultation, total amount, GST information, subsidies or discounts all conclude as 'yes'. Especially for GST, their descriptions may come in forms such as 'GST - ADD GST', 'GST - LESS GST'; for any descriptions that resemble these, conclude as 'yes'.\n",
    "    Considering the interactions between drugs, some drugs and treatments may not be designed for the patient's diagnoses and surgeries, but rather to counteract the side effects caused by other drugs. this scenario also needs to conclude as 'yes'\n",
    "    Analyse the exclusion details and verify if the treatment is relevant to the provided exclusion. if the treatment is relevant to the exclusion, conclude as 'no'. 'explanation' should be provided as the treatment is relevant to the exclusion list.\n",
    "    Conversely, consider the treatment medically unnecessary if it is ineffective, has safer alternatives, is discouraged by guidelines, or if risks outweigh benefits.\n",
    "   \n",
    "    ## Input: Given Medical Care: {treatments}.\n",
    "    ## ICD and Surgery Description list: {icd_surgery_description}.\n",
    "    ## Exclusion details: {exclusions}.\n",
    "   \n",
    "    ### Question: are all the given medical care at least a relevant testing or treatment for one of the items in the ICD and surgery descriptions list.\n",
    "   \n",
    "    ### Conclusion:\n",
    "    1. Explain the given result as a doctor.\n",
    "    2. Given different patient profiles and medical histories, provide a probability score without an explanation based on your analysis for the Yes/No binary output you provided to indicate the likelihood of this claim getting approved.\n",
    "   \n",
    "    ### Response: MUST Provide the output in json format with a key \"conclusion\" stating yes/no of the relevance, a key \"probability score\" providing a probability score (0-1 scale) to indicate the likelihood of this claim getting approved with 2 decimal places, and another key \"explanation\" stating the explanation as a doctor. Ensure that the conclusion is \"yes\" only if the probability score is at least 0.5. Ensure that the explanation is given in complete sentences without any truncations.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52b40054-d9aa-4ec4-9991-92346f807aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Prompt Optimization Process...\n",
      "📝 Original Prompt: ## Context:\n",
      "    Evaluate the relevance and necessity of all the provided medical care in relation to each item in the ICD and surgery descriptions list using your professional medical knowledge.\n",
      "   \n",
      "    ## Instruction:\n",
      "    For each item in the ICD and surgery descriptions list, determine:\n",
      "    If the medical care is necessary for diagnostic or examination purposes.\n",
      "    If the medical care is effective for the disease.\n",
      "    If the benefits of medical care outweigh any risks.\n",
      "    If the medical care is a standard practice for the diagnosis.\n",
      "    If the medical care is essential for the disease and not for cosmetic/lifestyle purposes.\n",
      "    If the medical care indirectly treats the disease.\n",
      "    Medical care that is follow-up visits, repeat visits, repeat consultation, total amount, GST information, subsidies or discounts all conclude as 'yes'. Especially for GST, their descriptions may come in forms such as 'GST - ADD GST', 'GST - LESS GST'; for any descriptions that resemble these, conclude as 'yes'.\n",
      "    Considering the interactions between drugs, some drugs and treatments may not be designed for the patient's diagnoses and surgeries, but rather to counteract the side effects caused by other drugs. this scenario also needs to conclude as 'yes'\n",
      "    Analyse the exclusion details and verify if the treatment is relevant to the provided exclusion. if the treatment is relevant to the exclusion, conclude as 'no'. 'explanation' should be provided as the treatment is relevant to the exclusion list.\n",
      "    Conversely, consider the treatment medically unnecessary if it is ineffective, has safer alternatives, is discouraged by guidelines, or if risks outweigh benefits.\n",
      "   \n",
      "    ## Input: Given Medical Care: {treatments}.\n",
      "    ## ICD and Surgery Description list: {icd_surgery_description}.\n",
      "    ## Exclusion details: {exclusions}.\n",
      "   \n",
      "    ### Question: are all the given medical care at least a relevant testing or treatment for one of the items in the ICD and surgery descriptions list.\n",
      "   \n",
      "    ### Conclusion:\n",
      "    1. Explain the given result as a doctor.\n",
      "    2. Given different patient profiles and medical histories, provide a probability score without an explanation based on your analysis for the Yes/No binary output you provided to indicate the likelihood of this claim getting approved.\n",
      "   \n",
      "    ### Response: MUST Provide the output in json format with a key \"conclusion\" stating yes/no of the relevance, a key \"probability score\" providing a probability score (0-1 scale) to indicate the likelihood of this claim getting approved with 2 decimal places, and another key \"explanation\" stating the explanation as a doctor. Ensure that the conclusion is \"yes\" only if the probability score is at least 0.5. Ensure that the explanation is given in complete sentences without any truncations.\n",
      "    \n",
      "================================================================================\n",
      "🤖 Agent 1: Generating test dataset...\n",
      "✅ Generated 12 test cases\n",
      "\n",
      "📋 Sample test cases:\n",
      "Case 1:\n",
      "  Input: {'treatments': 'Paracetamol 500mg PO q6h PRN for pain, Physical Therapy for lower back pain, Multivitamin daily', 'icd_surgery_description': 'M54.5 - Lower back pain, R51 - Headache', 'exclusions': 'Cosmetic procedures, experimental treatments'}\n",
      "  Expected: {'conclusion': 'yes', 'probability score': 0.9, 'explanation': 'Paracetamol is appropriate for pain management related to both lower back pain and headaches. Physical therapy is a standard treatment for lower back pain. The multivitamin is generally safe and supportive, though its direct impact on the specific ICD codes is less significant. None of these treatments are directly related to the exclusions.'}\n",
      "\n",
      "Case 2:\n",
      "  Input: {'treatments': 'Rhinoplasty, Botox injections, Lip fillers', 'icd_surgery_description': 'M54.5 - Lower back pain, R51 - Headache', 'exclusions': 'Cosmetic procedures, experimental treatments'}\n",
      "  Expected: {'conclusion': 'no', 'probability score': 0.1, 'explanation': 'Rhinoplasty, Botox injections, and lip fillers are cosmetic procedures and are not relevant to the provided ICD codes (lower back pain and headache). These treatments fall directly under the cosmetic procedures exclusion.'}\n",
      "\n",
      "Case 3:\n",
      "  Input: {'treatments': 'Chemotherapy (Cyclophosphamide), Radiation therapy, Ondansetron for nausea', 'icd_surgery_description': 'C50.9 - Breast cancer', 'exclusions': 'None'}\n",
      "  Expected: {'conclusion': 'yes', 'probability score': 0.95, 'explanation': \"Chemotherapy and radiation therapy are standard treatments for breast cancer. Ondansetron is used to manage nausea, a common side effect of chemotherapy. All the given treatments are relevant and necessary for the patient's diagnosis.\"}\n",
      "\n",
      "🤖 Agent 2: Executing prompt against test dataset...\n",
      "Processing test case 1/12... ✅\n",
      "Processing test case 2/12... ✅\n",
      "Processing test case 3/12... ✅\n",
      "Processing test case 4/12... ✅\n",
      "Processing test case 5/12... ✅\n",
      "Processing test case 6/12... ✅\n",
      "Processing test case 7/12... ✅\n",
      "Processing test case 8/12... ✅\n",
      "Processing test case 9/12... ✅\n",
      "Processing test case 10/12... ✅\n",
      "Processing test case 11/12... ✅\n",
      "Processing test case 12/12... ✅\n",
      "✅ Completed execution on 12 test cases\n",
      "🤖 Agent 3: Analyzing results and generating recommendations...\n",
      "The `input_variables` parameter is empty. Only the `response` column is used for computing this model-based metric.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-55ae30f1-716e-427f-b93f-d4e1fcc513da\" href=\"#view-view-vertex-resource-55ae30f1-716e-427f-b93f-d4e1fcc513da\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-55ae30f1-716e-427f-b93f-d4e1fcc513da');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/255766800726/locations/us-central1/metadataStores/default/contexts/prompt-optimization-experiment-346d7abe-63cd-44b2-8526-8e7d03a920dd to Experiment: prompt-optimization-experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-7007ada8-0576-4dc3-b250-b9b898e6443e\" href=\"#view-view-vertex-resource-7007ada8-0576-4dc3-b250-b9b898e6443e\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-7007ada8-0576-4dc3-b250-b9b898e6443e');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-346d7abe-63cd-44b2-8526-8e7d03a920dd?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/prompt-optimization-experiment/runs/prompt-optimization-experiment-346d7abe-63cd-44b2-8526-8e7d03a920dd?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 12 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 12 metric requests are successfully computed.\n",
      "Evaluation Took:2.000043304869905 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-bb755c2d-7acc-4dab-9f33-a1283a002860\" href=\"#view-view-vertex-resource-bb755c2d-7acc-4dab-9f33-a1283a002860\">\n",
       "          <span class=\"material-icons view-vertex-icon\">bar_chart</span>\n",
       "          <span>View evaluation results</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-bb755c2d-7acc-4dab-9f33-a1283a002860');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation');\n",
       "              } else {\n",
       "                window.open('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation completed\n",
      "📊 row_count: 12\n",
      "prompt_quality/mean: 4.916666666666667\n",
      "prompt_quality/std: 0.2886751345948129\n",
      "Success Rate: 100.0% (12/12)\n",
      "📝 Recommendations generated\n",
      "🤖 Agent 4: Validating enhancements and finalizing prompt...\n",
      "✅ Finalization approved\n",
      "💡 Reasoning: The recommendations are excellent, specific, and actionable. They address the identified issues of minor score discrepancies, reliance on implicit knowledge, and ambiguity in defining relevance. The proposed enhancements, particularly the inclusion of examples and more explicit instructions on handling partial relevance and conflicting information, are likely to improve the prompt's robustness and consistency. The formatting and structural improvements will also enhance readability and clarity. The recommendations are reasonable and not overly complicated, making them a valuable addition to the prompt engineering process. \n",
      "💾 Final prompt saved to readme_prompt.md\n",
      "\n",
      "================================================================================\n",
      "🎉 OPTIMIZATION COMPLETE!\n",
      "================================================================================\n",
      "📊 Final Results:\n",
      "   - Total Iterations: 1\n",
      "   - Test Cases: 12\n",
      "   - Success Rate: 12/12\n",
      "\n",
      "📝 Original Prompt:\n",
      "   ## Context:\n",
      "    Evaluate the relevance and necessity of all the provided medical care in relation to each item in the ICD and surgery descriptions list using your professional medical knowledge.\n",
      "   \n",
      "    ## Instruction:\n",
      "    For each item in the ICD and surgery descriptions list, determine:\n",
      "    If the medical care is necessary for diagnostic or examination purposes.\n",
      "    If the medical care is effective for the disease.\n",
      "    If the benefits of medical care outweigh any risks.\n",
      "    If the medical care is a standard practice for the diagnosis.\n",
      "    If the medical care is essential for the disease and not for cosmetic/lifestyle purposes.\n",
      "    If the medical care indirectly treats the disease.\n",
      "    Medical care that is follow-up visits, repeat visits, repeat consultation, total amount, GST information, subsidies or discounts all conclude as 'yes'. Especially for GST, their descriptions may come in forms such as 'GST - ADD GST', 'GST - LESS GST'; for any descriptions that resemble these, conclude as 'yes'.\n",
      "    Considering the interactions between drugs, some drugs and treatments may not be designed for the patient's diagnoses and surgeries, but rather to counteract the side effects caused by other drugs. this scenario also needs to conclude as 'yes'\n",
      "    Analyse the exclusion details and verify if the treatment is relevant to the provided exclusion. if the treatment is relevant to the exclusion, conclude as 'no'. 'explanation' should be provided as the treatment is relevant to the exclusion list.\n",
      "    Conversely, consider the treatment medically unnecessary if it is ineffective, has safer alternatives, is discouraged by guidelines, or if risks outweigh benefits.\n",
      "   \n",
      "    ## Input: Given Medical Care: {treatments}.\n",
      "    ## ICD and Surgery Description list: {icd_surgery_description}.\n",
      "    ## Exclusion details: {exclusions}.\n",
      "   \n",
      "    ### Question: are all the given medical care at least a relevant testing or treatment for one of the items in the ICD and surgery descriptions list.\n",
      "   \n",
      "    ### Conclusion:\n",
      "    1. Explain the given result as a doctor.\n",
      "    2. Given different patient profiles and medical histories, provide a probability score without an explanation based on your analysis for the Yes/No binary output you provided to indicate the likelihood of this claim getting approved.\n",
      "   \n",
      "    ### Response: MUST Provide the output in json format with a key \"conclusion\" stating yes/no of the relevance, a key \"probability score\" providing a probability score (0-1 scale) to indicate the likelihood of this claim getting approved with 2 decimal places, and another key \"explanation\" stating the explanation as a doctor. Ensure that the conclusion is \"yes\" only if the probability score is at least 0.5. Ensure that the explanation is given in complete sentences without any truncations.\n",
      "    \n",
      "\n",
      "🎯 Final Prompt:\n",
      "   ```\n",
      "## Context:\n",
      "Evaluate the relevance and necessity of provided medical care in relation to items in the ICD and surgery descriptions list, considering exclusion details. Use your professional medical knowledge and understanding of medical insurance claim processes.\n",
      "\n",
      "## Instructions:\n",
      "For *each* item in the ICD and surgery descriptions list and for *each* medical care provided, determine:\n",
      "\n",
      "1.  **Diagnostic/Examination Necessity:** Is the medical care required for diagnostic or examination purposes related to the ICD/surgery?\n",
      "2.  **Treatment Effectiveness:** Is the medical care effective for the disease/condition indicated by the ICD/surgery?\n",
      "3.  **Risk-Benefit Ratio:** Do the benefits of the medical care outweigh any potential risks for the patient?\n",
      "4.  **Standard Practice:** Is the medical care a standard and accepted practice for the diagnosis/condition?\n",
      "5.  **Essential vs. Cosmetic/Lifestyle:** Is the medical care essential for treating the disease, or is it primarily for cosmetic/lifestyle purposes?\n",
      "6.  **Direct vs. Indirect Treatment:** Does the medical care directly treat the disease, or does it indirectly support treatment (e.g., managing side effects, improving recovery)?\n",
      "7.  **Exclusion Relevance:** Is the medical care related to any of the provided exclusions?\n",
      "\n",
      "**Specific Guidance:**\n",
      "\n",
      "*   **GST/Follow-up Visits:** Medical care related to follow-up visits, repeat visits/consultations, total amounts, GST information, subsidies, or discounts should be considered relevant (\"yes\").  Specifically, descriptions like 'GST - ADD GST' or 'GST - LESS GST' should conclude as \"yes\".\n",
      "*   **Indirect Treatment (Examples):**\n",
      "    *   Ondansetron for nausea during chemotherapy.\n",
      "    *   Physical therapy after surgery to improve mobility and reduce pain.\n",
      "    *   Nutritional support for patients undergoing cancer treatment.\n",
      "*   **Drug Interactions:** Treatments designed to counteract the side effects of other drugs prescribed for the patient's diagnoses/surgeries should be considered relevant (\"yes\").\n",
      "*   **Exclusion Handling:**\n",
      "    *   If the treatment is *fully* relevant to an exclusion, conclude \"no\" and provide an explanation referencing the exclusion.\n",
      "    *   If the treatment has *both* therapeutic and cosmetic applications (e.g., certain types of surgery), assess the *primary* intention based on the ICD code and patient context. If the *primary* intention is cosmetic, conclude \"no\".\n",
      "*   **Conflicting Information:** In cases where standard practice conflicts with the specific patient's condition, prioritize the patient's condition and the latest medical guidelines. For example, a standard pain medication might be contraindicated due to a patient allergy.\n",
      "*   **Relevance Definition:** Relevance implies that the medical care has a logical and justifiable connection to the diagnosis or procedure. This includes direct treatment, necessary diagnostic procedures, and supportive care that directly contributes to the patient's recovery or well-being.\n",
      "\n",
      "**Conversely, consider the treatment medically unnecessary if it is ineffective, has safer alternatives, is discouraged by guidelines, or if risks outweigh benefits.**\n",
      "\n",
      "## Input:\n",
      "Given Medical Care: {treatments}\n",
      "ICD and Surgery Description list: {icd_surgery_description}\n",
      "Exclusion details: {exclusions}\n",
      "\n",
      "## Question:\n",
      "Considering *all* the given medical care and *all* the ICD and surgery descriptions, is *at least one* treatment relevant and necessary for *at least one* of the diagnoses or procedures, and not excluded?\n",
      "\n",
      "## Conclusion:\n",
      "1.  Explain your reasoning as a doctor, justifying your conclusion based on medical knowledge, the provided ICD codes, and exclusion details.\n",
      "2.  Provide a probability score (0.00-1.00) indicating the likelihood of this claim being approved by medical insurance, considering both medical necessity and typical insurance coverage policies. Do NOT provide an explanation for the probability score.\n",
      "\n",
      "## Response:\n",
      "MUST provide the output in JSON format with the following keys:\n",
      "\n",
      "*   `\"conclusion\"`: A string, either \"yes\" or \"no\", indicating the relevance of the medical care.  Ensure that the conclusion is \"yes\" only if the probability score is at least 0.5.\n",
      "*   `\"probability score\"`: A number between 0.00 and 1.00 (inclusive), formatted to two decimal places, representing the likelihood of claim approval.\n",
      "*   `\"explanation\"`: A string providing a detailed explanation of your reasoning as a doctor, in complete sentences without truncations.\n",
      "\n",
      "**Example 1: Partial Relevance (Hypothetical):**\n",
      "**Input:**\n",
      "Given Medical Care: Botox injections for migraines and forehead wrinkles.\n",
      "ICD and Surgery Description list: G43.909 - Migraine, unspecified, not intractable\n",
      "Exclusion details: Cosmetic procedures\n",
      "\n",
      "**Expected Output:**\n",
      "```json\n",
      "{\n",
      "  \"conclusion\": \"yes\",\n",
      "  \"probability score\": \"0.65\",\n",
      "  \"explanation\": \"Botox injections are an accepted treatment for chronic migraines (G43.909). While Botox is also used for cosmetic purposes like reducing forehead wrinkles, in this case, the presence of a migraine diagnosis suggests the treatment is medically necessary. Therefore, the treatment is considered relevant despite the cosmetic application.\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Example 2: Irrelevant Treatment (Hypothetical):**\n",
      "**Input:**\n",
      "Given Medical Care: Amoxicillin\n",
      "ICD and Surgery Description list: M25.561 - Pain in right knee\n",
      "Exclusion details: Antibiotics for viral infections\n",
      "\n",
      "**Expected Output:**\n",
      "```json\n",
      "{\n",
      "  \"conclusion\": \"no\",\n",
      "  \"probability score\": \"0.10\",\n",
      "  \"explanation\": \"Amoxicillin is an antibiotic used to treat bacterial infections. Pain in the right knee (M25.561) is typically associated with musculoskeletal issues, not bacterial infections. Therefore, Amoxicillin is not a relevant treatment for this condition.\"\n",
      "}\n",
      "```\n",
      "```\n",
      "\n",
      "💾 Detailed results saved to: readme_prompt.md\n",
      "\n",
      "✅ Process completed!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTION EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "# Example usage\n",
    "user_prompt = \"\"\"\n",
    "You are a helpful assistant that writes creative product descriptions for e-commerce. \n",
    "Given a product name and basic features, write an engaging product description that highlights key benefits and appeals to customers.\n",
    "\"\"\"\n",
    "\n",
    "print(\"🚀 Starting Prompt Optimization Process...\")\n",
    "print(f\"📝 Original Prompt: {user_prompt}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize state\n",
    "initial_state = {\n",
    "    \"original_prompt\": user_prompt,\n",
    "    \"current_prompt\": user_prompt,\n",
    "    \"test_dataset\": [],\n",
    "    \"agent2_results\": [],\n",
    "    \"evaluation_results\": {},\n",
    "    \"enhancement_recommendations\": \"\",\n",
    "    \"enhancement_makes_sense\": False,\n",
    "    \"final_prompt\": \"\",\n",
    "    \"iteration\": 1,\n",
    "    \"max_iterations\": 3\n",
    "}\n",
    "\n",
    "# Run the workflow\n",
    "try:\n",
    "    final_state = app.invoke(initial_state)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎉 OPTIMIZATION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"📊 Final Results:\")\n",
    "    print(f\"   - Total Iterations: {final_state.get('iteration', 1)}\")\n",
    "    print(f\"   - Test Cases: {len(final_state.get('test_dataset', []))}\")\n",
    "    print(f\"   - Success Rate: {final_state.get('evaluation_results', {}).get('success_cases', 0)}/{final_state.get('evaluation_results', {}).get('total_cases', 0)}\")\n",
    "    \n",
    "    print(f\"\\n📝 Original Prompt:\")\n",
    "    print(f\"   {final_state['original_prompt']}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Final Prompt:\")\n",
    "    print(f\"   {final_state.get('final_prompt', final_state['current_prompt'])}\")\n",
    "    \n",
    "    if os.path.exists(\"readme_prompt.md\"):\n",
    "        print(f\"\\n💾 Detailed results saved to: readme_prompt.md\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during execution: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n✅ Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f3e53-5229-4ef6-be4c-34e631c46a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py312",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "conda-base-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
