{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0Wchgzqer8F"
   },
   "source": [
    "### Install Vertex AI Python SDK for Gen AI Evaluation Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T-Cgoq37er8F"
   },
   "outputs": [],
   "source": [
    "# %pip install --upgrade --user --quiet google-cloud-aiplatform[evaluation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOzpVrBLer8F"
   },
   "source": [
    "### Restart runtime\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5i8OM_85er8F"
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8NVZ7z6er8G"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiYxfbBJer8G"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MgUI8ziHer8G"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka4wP_ljer8G"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import JSON\n",
    "import json\n",
    "\n",
    "# Cloud project id.\n",
    "PROJECT_IDS = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_IDS[0]  # @param {type:\"string\"}\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"TRUE\" # Use Vertex AI API\n",
    "# [your-project-id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N-i6OtB9er8G"
   },
   "outputs": [],
   "source": [
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "EXPERIMENT_NAME = \"my-eval-task-experiment\"  # @param {type:\"string\"}\n",
    "\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    raise ValueError(\"Please set your PROJECT_ID\")\n",
    "\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvhI92xhQTzk"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qP4ihOCkEBje"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vertexai.evaluation import EvalTask, PointwiseMetric, PointwiseMetricPromptTemplate\n",
    "from vertexai.preview.evaluation import notebook_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo7ahGbnnYfp"
   },
   "source": [
    "# Set up eval metrics based on your criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aG3kUfTmoAwb"
   },
   "source": [
    "As a developer, you would like to evaluate the text quality generated from an LLM based on two criteria: Fluency and Entertaining. You define a metric called `text_quality` using those two criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EGe_vlUvPVOM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `input_variables` parameter is empty. Only the `response` column is used for computing this model-based metric.\n"
     ]
    }
   ],
   "source": [
    "# Your own definition of text_quality.\n",
    "text_quality = PointwiseMetric(\n",
    "    metric=\"text_quality\",\n",
    "    metric_prompt_template=PointwiseMetricPromptTemplate(\n",
    "        criteria={\n",
    "            \"fluency\": \"Sentences flow smoothly and are easy to read, avoiding awkward phrasing or run-on sentences. Ideas and sentences connect logically, using transitions effectively where needed.\",\n",
    "            \"entertaining\": \"Short, amusing text that incorporates emojis, exclamations and questions to convey quick and spontaneous communication and diversion.\",\n",
    "        },\n",
    "        rating_rubric={\n",
    "            \"1\": \"The response performs well on both criteria.\",\n",
    "            \"0\": \"The response is somewhat aligned with both criteria\",\n",
    "            \"-1\": \"The response falls short on both criteria\",\n",
    "        },\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "007cf682dfc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instruction\n",
      "You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models. We will provide you with the user prompt and an AI-generated responses.\n",
      "You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n",
      "You will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step by step explanations for your rating, and only choose ratings from the Rating Rubric.\n",
      "\n",
      "\n",
      "# Evaluation\n",
      "## Criteria\n",
      "entertaining: Short, amusing text that incorporates emojis, exclamations and questions to convey quick and spontaneous communication and diversion.\n",
      "fluency: Sentences flow smoothly and are easy to read, avoiding awkward phrasing or run-on sentences. Ideas and sentences connect logically, using transitions effectively where needed.\n",
      "\n",
      "## Rating Rubric\n",
      "-1: The response falls short on both criteria\n",
      "0: The response is somewhat aligned with both criteria\n",
      "1: The response performs well on both criteria.\n",
      "\n",
      "## Evaluation Steps\n",
      "Step 1: Assess the response in aspects of all criteria provided. Provide assessment according to each criterion.\n",
      "Step 2: Score based on the rating rubric. Give a brief rationale to explain your evaluation considering each individual criterion.\n",
      "\n",
      "\n",
      "# User Inputs and AI-generated Response\n",
      "## User Inputs\n",
      "\n",
      "\n",
      "\n",
      "## AI-generated Response\n",
      "{response}\n"
     ]
    }
   ],
   "source": [
    "print(text_quality.metric_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xITUrMB_nbsg"
   },
   "source": [
    "# Prepare your dataset\n",
    "\n",
    "Evaluate stored generative AI model responses in an evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XGY40wjrQWOc"
   },
   "outputs": [],
   "source": [
    "responses = [\n",
    "    # An example of good text_quality\n",
    "    \"Life is a rollercoaster, full of ups and downs, but it's the thrill that keeps us coming back for more!\",\n",
    "    # An example of medium text_quality\n",
    "    \"The weather is nice today, not too hot, not too cold.\",\n",
    "    # An example of poor text_quality\n",
    "    \"The weather is, you know, whatever.\",\n",
    "]\n",
    "\n",
    "eval_dataset = pd.DataFrame(\n",
    "    {\n",
    "        \"response\": responses,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJg5FdWfnhjz"
   },
   "source": [
    "# Run evaluation\n",
    "\n",
    "With the evaluation dataset and metrics defined, you can run evaluation for an `EvalTask` on different models and applications, prompt templates, and many other use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rOvo-LpsQTIj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-86f8179f-7499-4ab4-a979-073c2adffd28\" href=\"#view-view-vertex-resource-86f8179f-7499-4ab4-a979-073c2adffd28\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-86f8179f-7499-4ab4-a979-073c2adffd28');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/my-eval-task-experiment/runs?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/my-eval-task-experiment/runs?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/255766800726/locations/us-central1/metadataStores/default/contexts/my-eval-task-experiment-68035fa5-eb48-4cec-9bc0-f0a21d825354 to Experiment: my-eval-task-experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-ab1f3c84-3e51-435c-bbbc-b2abfd3cbc1d\" href=\"#view-view-vertex-resource-ab1f3c84-3e51-435c-bbbc-b2abfd3cbc1d\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-ab1f3c84-3e51-435c-bbbc-b2abfd3cbc1d');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/my-eval-task-experiment/runs/my-eval-task-experiment-68035fa5-eb48-4cec-9bc0-f0a21d825354?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/my-eval-task-experiment/runs/my-eval-task-experiment-68035fa5-eb48-4cec-9bc0-f0a21d825354?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 3 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 3 metric requests are successfully computed.\n",
      "Evaluation Took:0.9326070339884609 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-a0ecabeb-fa61-4d9c-9280-ec23ae398771\" href=\"#view-view-vertex-resource-a0ecabeb-fa61-4d9c-9280-ec23ae398771\">\n",
       "          <span class=\"material-icons view-vertex-icon\">bar_chart</span>\n",
       "          <span>View evaluation results</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-a0ecabeb-fa61-4d9c-9280-ec23ae398771');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation');\n",
       "              } else {\n",
       "                window.open('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_result = EvalTask(\n",
    "    dataset=eval_dataset, metrics=[text_quality], experiment=EXPERIMENT_NAME\n",
    ").evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwnZMsXBnjS7"
   },
   "source": [
    "You can view the summary metrics and row-based metrics for each response in the `EvalResult`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2QOFq9YZROPr"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Summary Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_count</th>\n",
       "      <th>text_quality/mean</th>\n",
       "      <th>text_quality/std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_count  text_quality/mean  text_quality/std\n",
       "0        3.0                0.0               0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Row-based Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>text_quality/explanation</th>\n",
       "      <th>text_quality/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Life is a rollercoaster, full of ups and downs...</td>\n",
       "      <td>The response has a basic level of fluency, but...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The weather is nice today, not too hot, not to...</td>\n",
       "      <td>The response is somewhat aligned with the flue...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The weather is, you know, whatever.</td>\n",
       "      <td>The response is not particularly entertaining ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  Life is a rollercoaster, full of ups and downs...   \n",
       "1  The weather is nice today, not too hot, not to...   \n",
       "2                The weather is, you know, whatever.   \n",
       "\n",
       "                            text_quality/explanation  text_quality/score  \n",
       "0  The response has a basic level of fluency, but...                 0.0  \n",
       "1  The response is somewhat aligned with the flue...                 0.0  \n",
       "2  The response is not particularly entertaining ...                 0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_utils.display_eval_result(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.preview.evaluation import (\n",
    "    CustomOutputConfig,\n",
    "    EvalTask,\n",
    "    PointwiseMetric,\n",
    "    PredefinedRubricMetrics,\n",
    "    RubricBasedMetric,\n",
    "    RubricGenerationConfig,\n",
    "    notebook_utils,\n",
    "    utils,\n",
    "    MetricPromptTemplateExamples,\n",
    ")\n",
    "\n",
    "from vertexai.evaluation import (\n",
    "    PairwiseMetricPromptTemplate,\n",
    "    PointwiseMetric,\n",
    "    PointwiseMetricPromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coherence',\n",
       " 'fluency',\n",
       " 'safety',\n",
       " 'groundedness',\n",
       " 'instruction_following',\n",
       " 'verbosity',\n",
       " 'text_quality',\n",
       " 'summarization_quality',\n",
       " 'question_answering_quality',\n",
       " 'multi_turn_chat_quality',\n",
       " 'multi_turn_safety',\n",
       " 'pairwise_coherence',\n",
       " 'pairwise_fluency',\n",
       " 'pairwise_safety',\n",
       " 'pairwise_groundedness',\n",
       " 'pairwise_instruction_following',\n",
       " 'pairwise_verbosity',\n",
       " 'pairwise_text_quality',\n",
       " 'pairwise_summarization_quality',\n",
       " 'pairwise_question_answering_quality',\n",
       " 'pairwise_multi_turn_chat_quality',\n",
       " 'pairwise_multi_turn_safety']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MetricPromptTemplateExamples.list_example_metric_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Instruction\n",
      "You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\n",
      "We will provide you with the user input and an AI-generated response.\n",
      "You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the criteria provided in the Evaluation section below.\n",
      "You will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step by step explanations for your rating, and only choose ratings from the Rating Rubric.\n",
      "\n",
      "# Evaluation\n",
      "## Metric Definition\n",
      "You will be assessing fluency, which measures language mastery of the model's response based on the user prompt.\n",
      "\n",
      "## Criteria\n",
      "Fluency: The text is free of grammatical errors, employs varied sentence structures, and maintains a consistent tone and style, resulting in a smooth and natural flow that is easy to understand.\n",
      "\n",
      "## Rating Rubric\n",
      "5 (completely fluent): The response is free of grammatical errors, demonstrates nuanced word choice, and has a natural, seamless flow.\n",
      "4 (mostly fluent): The response has very few, if any, minor grammatical errors. Word choice is clear, and sentences generally flow well.\n",
      "3 (somewhat fluent): The response has grammatical errors present, which may cause some difficulty for the reader. Word choice is mostly appropriate, but some awkward phrasing or word repetition may exist.\n",
      "2 (somewhat inarticulate): The response has frequent grammatical errors that make the writing difficult to understand. Sentence structure is often awkward, and there's little sense of flow.\n",
      "1 (inarticulate): The response is riddled with grammatical issues, rendering it incomprehensible in parts. Word choices may be very limited or inaccurate.\n",
      "\n",
      "## Evaluation Steps\n",
      "STEP 1: Assess grammar correctness: Identify any specific errors in the response's sentence structure, verb usage, subject-verb agreement, punctuation, and capitalization.\n",
      "STEP 2: Assess word choice and flow: Examine the response's sentence structure and how the writing moves from one idea to the next. Are words accurate and well-suited to the context?\n",
      "STEP 3: Assess overall cohesion: Does the entire response read logically and smoothly, with appropriate transitions?\n",
      "\n",
      "\n",
      "# User Inputs and AI-generated Response\n",
      "## User Inputs\n",
      "### Prompt\n",
      "{prompt}\n",
      "\n",
      "## AI-generated Response\n",
      "{response}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(MetricPromptTemplateExamples.get_prompt_template('fluency'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-e19d4775-f31c-4fce-bfa0-82ae59eb7d9b\" href=\"#view-view-vertex-resource-e19d4775-f31c-4fce-bfa0-82ae59eb7d9b\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-e19d4775-f31c-4fce-bfa0-82ae59eb7d9b');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/my-eval-task-experiment/runs?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/my-eval-task-experiment/runs?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/255766800726/locations/us-central1/metadataStores/default/contexts/my-eval-task-experiment-d53e59a4-00a4-4e3e-ad48-1949208cd1f8 to Experiment: my-eval-task-experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-ccf3bc04-c70c-4489-8bed-ddca4ade3fc3\" href=\"#view-view-vertex-resource-ccf3bc04-c70c-4489-8bed-ddca4ade3fc3\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-ccf3bc04-c70c-4489-8bed-ddca4ade3fc3');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/my-eval-task-experiment/runs/my-eval-task-experiment-d53e59a4-00a4-4e3e-ad48-1949208cd1f8?project=my-project-0004-346516');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/my-eval-task-experiment/runs/my-eval-task-experiment-d53e59a4-00a4-4e3e-ad48-1949208cd1f8?project=my-project-0004-346516', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 3 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Metric name: fluency is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertexai/evaluation/metrics/_instance_evaluation.py:112\u001b[0m, in \u001b[0;36mbuild_request\u001b[0;34m(metric, row_dict, evaluation_run_config)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     metric_spec \u001b[38;5;241m=\u001b[39m \u001b[43m_METRIC_NAME_TO_METRIC_SPEC\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fluency'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvalTask\n\u001b[1;32m      3\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mEvalTask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMetricPromptTemplateExamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPointwise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFLUENCY\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEXPERIMENT_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model=MODEL,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# experiment_run=EXPERIMENT_RUN_NAME,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertexai/evaluation/eval_task.py:443\u001b[0m, in \u001b[0;36mEvalTask.evaluate\u001b[0;34m(self, model, prompt_template, experiment_run_name, response_column_name, baseline_model_response_column_name, evaluation_service_qps, retry_timeout, output_file_name)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment \u001b[38;5;129;01mand\u001b[39;00m global_experiment_name:\n\u001b[1;32m    440\u001b[0m     metadata\u001b[38;5;241m.\u001b[39m_experiment_tracker\u001b[38;5;241m.\u001b[39mset_experiment(\n\u001b[1;32m    441\u001b[0m         experiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment, backing_tensorboard\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m     eval_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_with_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_run_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_run_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluation_service_qps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation_service_qps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m     metadata\u001b[38;5;241m.\u001b[39m_experiment_tracker\u001b[38;5;241m.\u001b[39mset_experiment(\n\u001b[1;32m    452\u001b[0m         experiment\u001b[38;5;241m=\u001b[39mglobal_experiment_name,\n\u001b[1;32m    453\u001b[0m         backing_tensorboard\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    454\u001b[0m         display_button\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    455\u001b[0m     )\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_experiment_name:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertexai/evaluation/eval_task.py:357\u001b[0m, in \u001b[0;36mEvalTask._evaluate_with_experiment\u001b[0;34m(self, model, prompt_template, experiment_run_name, evaluation_service_qps, retry_timeout, output_file_name)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vertexai\u001b[38;5;241m.\u001b[39mpreview\u001b[38;5;241m.\u001b[39mstart_run(experiment_run_name):\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_eval_experiment_param(\n\u001b[1;32m    353\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    354\u001b[0m         prompt_template\u001b[38;5;241m=\u001b[39mprompt_template,\n\u001b[1;32m    355\u001b[0m         output_file_name\u001b[38;5;241m=\u001b[39moutput_file_name,\n\u001b[1;32m    356\u001b[0m     )\n\u001b[0;32m--> 357\u001b[0m     eval_result \u001b[38;5;241m=\u001b[39m \u001b[43m_evaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_column_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_column_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluation_service_qps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation_service_qps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m     eval_result\u001b[38;5;241m.\u001b[39msummary_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    368\u001b[0m         k: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(v) \u001b[38;5;28;01melse\u001b[39;00m v)\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m eval_result\u001b[38;5;241m.\u001b[39msummary_metrics\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    370\u001b[0m     }\n\u001b[1;32m    371\u001b[0m     eval_result\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment,\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment_run\u001b[39m\u001b[38;5;124m\"\u001b[39m: experiment_run_name,\n\u001b[1;32m    374\u001b[0m     }\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertexai/evaluation/_evaluation.py:976\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, model, prompt_template, metric_column_mapping, evaluation_service_qps, retry_timeout)\u001b[0m\n\u001b[1;32m    974\u001b[0m _validate_metric_column_map(evaluation_run_config)\n\u001b[1;32m    975\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 976\u001b[0m evaluation_result \u001b[38;5;241m=\u001b[39m \u001b[43m_compute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation_run_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    978\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Took:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt2\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertexai/evaluation/_evaluation.py:768\u001b[0m, in \u001b[0;36m_compute_metrics\u001b[0;34m(evaluation_run_config)\u001b[0m\n\u001b[1;32m    763\u001b[0m instance_list\u001b[38;5;241m.\u001b[39mappend(row_dict)\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m api_metrics:\n\u001b[1;32m    765\u001b[0m     future \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    766\u001b[0m         _instance_evaluation\u001b[38;5;241m.\u001b[39mevaluate_instances,\n\u001b[1;32m    767\u001b[0m         client\u001b[38;5;241m=\u001b[39mevaluation_run_config\u001b[38;5;241m.\u001b[39mclient,\n\u001b[0;32m--> 768\u001b[0m         request\u001b[38;5;241m=\u001b[39m\u001b[43m_instance_evaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevaluation_run_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation_run_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    773\u001b[0m         rate_limiter\u001b[38;5;241m=\u001b[39mrate_limiter,\n\u001b[1;32m    774\u001b[0m         retry_timeout\u001b[38;5;241m=\u001b[39mevaluation_run_config\u001b[38;5;241m.\u001b[39mretry_timeout,\n\u001b[1;32m    775\u001b[0m     )\n\u001b[1;32m    776\u001b[0m     future\u001b[38;5;241m.\u001b[39madd_done_callback(\u001b[38;5;28;01mlambda\u001b[39;00m _: pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    777\u001b[0m     futures_by_metric[metric]\u001b[38;5;241m.\u001b[39mappend((future, idx))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/vertexai/evaluation/metrics/_instance_evaluation.py:114\u001b[0m, in \u001b[0;36mbuild_request\u001b[0;34m(metric, row_dict, evaluation_run_config)\u001b[0m\n\u001b[1;32m    112\u001b[0m     metric_spec \u001b[38;5;241m=\u001b[39m _METRIC_NAME_TO_METRIC_SPEC[metric_name]\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    115\u001b[0m model_based_metric_instance_input \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    116\u001b[0m metric_column_mapping \u001b[38;5;241m=\u001b[39m evaluation_run_config\u001b[38;5;241m.\u001b[39mmetric_column_mapping\n",
      "\u001b[0;31mValueError\u001b[0m: Metric name: fluency is not supported."
     ]
    }
   ],
   "source": [
    "# from vertexai.evaluation import EvalTask\n",
    "\n",
    "# eval_result = EvalTask(\n",
    "#     dataset=eval_dataset,\n",
    "#     metrics=[MetricPromptTemplateExamples.Pointwise.FLUENCY],\n",
    "#     experiment=EXPERIMENT_NAME,\n",
    "# ).evaluate(\n",
    "#     # model=MODEL,\n",
    "#     # experiment_run=EXPERIMENT_RUN_NAME,\n",
    "# )\n",
    "\n",
    "\n",
    "# Define a pointwise multi-turn chat quality metric\n",
    "pointwise_chat_quality_metric_prompt = \"\"\"Evaluate the AI's contribution to a meaningful conversation, considering coherence, fluency, groundedness, and conciseness.\n",
    " Review the chat history for context. Rate the response on a 1-5 scale, with explanations for each criterion and its overall impact.\n",
    "\n",
    "# Conversation History\n",
    "{history}\n",
    "\n",
    "# Current User Prompt\n",
    "{prompt}\n",
    "\n",
    "# AI-generated Response\n",
    "{response}\n",
    "\"\"\"\n",
    "\n",
    "freeform_multi_turn_chat_quality_metric = PointwiseMetric(\n",
    "    metric=\"multi_turn_chat_quality_metric\",\n",
    "    metric_prompt_template=pointwise_chat_quality_metric_prompt,\n",
    ")\n",
    "\n",
    "# Run evaluation using the freeform_multi_turn_chat_quality_metric metric\n",
    "eval_task = EvalTask(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[freeform_multi_turn_chat_quality_metric],\n",
    ")\n",
    "eval_result = eval_task.evaluate(\n",
    "    model=MODEL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLMKgL2OnCyt"
   },
   "source": [
    "# Clean up\n",
    "\n",
    "Delete ExperimentRun created by the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Dv8drshKnEf2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.cloud import aiplatform\n",
    "\n",
    "# aiplatform.ExperimentRun(\n",
    "#     run_name=eval_result.metadata[\"experiment_run\"],\n",
    "#     experiment=eval_result.metadata[\"experiment\"],\n",
    "# ).delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_to_gen_ai_evaluation_service_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py312",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "conda-base-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
